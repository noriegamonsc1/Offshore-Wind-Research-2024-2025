{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8aced5-3300-4de4-b917-71cde62c8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming all_arrays is a list of tuples (speed, direction, lat, lon, date)\n",
    "# Flatten the data and add date information\n",
    "flattened_data_list = []\n",
    "for speed, direction, lat, lon, date in all_arrays:\n",
    "    valid_mask = ~np.isnan(speed)\n",
    "    speed_valid = speed[valid_mask]\n",
    "    direction_valid = direction[valid_mask]\n",
    "    lat_valid = lat[valid_mask]\n",
    "    lon_valid = lon[valid_mask]\n",
    "    date_valid = date[valid_mask]  # Ensure date matches\n",
    "\n",
    "    # Convert direction to Cartesian coordinates\n",
    "    dir_cos = np.cos(direction_valid * np.pi / 180)\n",
    "    dir_sin = np.sin(direction_valid * np.pi / 180)\n",
    "\n",
    "    # Combine into a single array (adding the date as a feature)\n",
    "    flattened_data = np.column_stack((speed_valid, dir_cos, dir_sin, lat_valid, lon_valid, date_valid))\n",
    "    flattened_data_list.append(flattened_data)\n",
    "\n",
    "# Concatenate all flattened arrays into a single array\n",
    "all_flattened_data = np.concatenate(flattened_data_list, axis=0)\n",
    "\n",
    "# Create a DataFrame for sorting and managing dates\n",
    "df = pd.DataFrame(all_flattened_data, columns=['speed', 'dir_cos', 'dir_sin', 'lat', 'lon', 'date'])\n",
    "df['date'] = pd.to_datetime(df['date'])  # Ensure the date is in datetime format\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "df.sort_values('date', inplace=True)\n",
    "\n",
    "# Normalize features except date\n",
    "scaler = StandardScaler()\n",
    "df[['speed', 'dir_cos', 'dir_sin', 'lat', 'lon']] = scaler.fit_transform(df[['speed', 'dir_cos', 'dir_sin', 'lat', 'lon']])\n",
    "\n",
    "# Create sequences based on time order\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        seq = data[i:i + seq_length]  # Get the current sequence\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Create sequences from normalized data (ignoring date for model input)\n",
    "sequences = create_sequences(df[['speed', 'dir_cos', 'dir_sin', 'lat', 'lon']].to_numpy(), sequence_length=3)\n",
    "\n",
    "# Split the sequences into training and validation sets\n",
    "train_data, val_data = train_test_split(sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the split data into PyTorch tensors\n",
    "train_tensor = torch.FloatTensor(train_data)\n",
    "val_tensor = torch.FloatTensor(val_data)\n",
    "\n",
    "# Define the LSTM Autoencoder class as before (RecurrentAutoencoder)\n",
    "class RecurrentAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(RecurrentAutoencoder, self).__init__()\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(hidden_dim, input_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded_seq, (h, c) = self.encoder_lstm(x)\n",
    "        decoded_seq, _ = self.decoder_lstm(encoded_seq)\n",
    "        return decoded_seq\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = RecurrentAutoencoder(input_dim=5, hidden_dim=4)  # Adjust hidden_dim as needed\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 600\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(train_tensor)\n",
    "    train_loss = criterion(output, train_tensor)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_losses.append(train_loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(val_tensor)\n",
    "        val_loss = criterion(val_output, val_tensor)\n",
    "\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "# Plot the training and validation losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "747ce89c-1525-4f8e-9dec-22a9b4b36ed2",
   "metadata": {},
   "source": [
    "Key Changes Explained\n",
    "Adding Date Information: When flattening your data, ensure that each data point includes the date it was captured.\n",
    "\n",
    "Sorting by Date: Create a DataFrame from your flattened data and sort it by date. This ensures that you are processing your observations in chronological order.\n",
    "\n",
    "Creating Sequences: The create_sequences function still creates overlapping sequences based on the ordered data. This allows you to maintain a temporal relationship between the observations.\n",
    "\n",
    "Using Only Features for the Model: When creating sequences for the model input, you exclude the date and only use speed, direction, latitude, and longitude as features.\n",
    "\n",
    "Conclusion\n",
    "This approach allows you to leverage the temporal aspect of your data effectively, even when not all observations are from the same geographical location. Each sequence will contain data from the ordered time series, enabling the LSTM to learn patterns over time. If you have further questions or need additional clarification, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52a1c9-3614-47b8-aad0-7a398ccba6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming all_arrays contains tuples of (speed, direction, lat, lon, date)\n",
    "flattened_data_list = []\n",
    "for speed, direction, lat, lon, date in all_arrays:\n",
    "    valid_mask = ~np.isnan(speed)\n",
    "    speed_valid = speed[valid_mask]\n",
    "    direction_valid = direction[valid_mask]\n",
    "    lat_valid = lat[valid_mask]\n",
    "    lon_valid = lon[valid_mask]\n",
    "    date_valid = date[valid_mask]\n",
    "\n",
    "    # Convert direction to Cartesian coordinates\n",
    "    dir_cos = np.cos(direction_valid * np.pi / 180)\n",
    "    dir_sin = np.sin(direction_valid * np.pi / 180)\n",
    "\n",
    "    # Flattening and combining data\n",
    "    flattened_data = np.column_stack((speed_valid, dir_cos, dir_sin, lat_valid, lon_valid, date_valid))\n",
    "    flattened_data_list.append(flattened_data)\n",
    "\n",
    "# Create a DataFrame from flattened data\n",
    "df = pd.DataFrame(np.vstack(flattened_data_list), columns=['speed', 'dir_cos', 'dir_sin', 'lat', 'lon', 'date'])\n",
    "\n",
    "# Convert the date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "df.sort_values('date', inplace=True)\n",
    "\n",
    "# Drop the date column (not needed for training)\n",
    "df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df.values)\n",
    "\n",
    "# Decide on a sequence length (e.g., 10 time steps)\n",
    "sequence_length = 10\n",
    "\n",
    "# Prepare the data for LSTM\n",
    "X = []\n",
    "for i in range(len(scaled_data) - sequence_length):\n",
    "    X.append(scaled_data[i:i + sequence_length])\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_data, val_data = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now you can feed train_data and val_data into your LSTM Autoencoder\n",
    "train_tensor = torch.FloatTensor(train_data)\n",
    "val_tensor = torch.FloatTensor(val_data)\n",
    "\n",
    "# Continue with your LSTM Autoencoder training...\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "721dd465-4d07-47de-80d4-2c98a70b3506",
   "metadata": {},
   "source": [
    "Important Notes\n",
    "Sequence Length: The choice of sequence_length is important and should be based on how many previous time steps you believe are relevant for predicting or reconstructing the current time step.\n",
    "Data Normalization: Normalizing your features helps improve the performance of neural networks by ensuring that the input data is on a similar scale.\n",
    "Validation: Ensure that you have a sufficient number of samples after creating sequences, as your final dataset will have reduced dimensions.\n",
    "By following these steps, you should be well-prepared to feed your data into the LSTM Autoencoder and start training it to capture the temporal dependencies in your dataset! If you have any more questions or need further clarification on any of these steps, feel free to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18255a-0d51-4b05-903f-46daecc2922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences based on existing order\n",
    "sequence_length = 10\n",
    "X = []\n",
    "for i in range(len(scaled_data) - sequence_length):\n",
    "    X.append(scaled_data[i:i + sequence_length])\n",
    "\n",
    "X = np.array(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cris_env)",
   "language": "python",
   "name": "cris_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
