{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d7f266-dbf7-4a06-b8e8-dcf264cfeef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-03 17:27:24.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moffshore_wind_nj.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: C:\\Users\\crist\\OneDrive - montclair.edu\\00.MSU_MsDataScience\\2024.FALL(Offshore_research)\\03.CODING\\00.CCNM_OffshoreWind2425(Github)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 files\n",
      "Scaled data list: 11\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "\n",
    "from offshore_wind_nj.data_loader import data_files, load_data, all_arrays, extract_datetime_from_filename, removed_files, load_single_data\n",
    "from offshore_wind_nj.sar_wind_plot import plot_wind_field, plot_wind_field_by_arrays\n",
    "from offshore_wind_nj.data_cleaning import fill_zeros, find_zeros, fill_nan, find_nan\n",
    "from offshore_wind_nj.data_processing import flattened_data_list, scaled_data_list, fill_nan_all_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0932389c-8495-4411-913c-8e6fa6e1cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill_nan_all_arrays(all_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f59dbeaa-b01c-4f63-b504-42ce33036bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data_list = []\n",
    "for speed, direction, lat, lon in all_arrays:\n",
    "    # Assuming speed, direction, lat, lon are of shape (H, W) after NaN removal\n",
    "    # Convert direction to Cartesian coordinates\n",
    "    dir_cos = np.cos(np.radians(direction))  # Shape (H, W)\n",
    "    dir_sin = np.sin(np.radians(direction))  # Shape (H, W)\n",
    "\n",
    "    # Stack all values for the same pixel (flattening)\n",
    "    flattened_data = np.column_stack((speed.flatten(), dir_cos.flatten(), dir_sin.flatten(), lat.flatten(), lon.flatten()))\n",
    "    flattened_data_list.append(flattened_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77ddd83e-c199-43a4-a64e-9ebb0c7dc32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.112082  ,  -0.9799002 ,  -0.1994884 ,  35.73571   ,\n",
       "        -74.00143   ],\n",
       "       [  8.611857  ,  -0.9798779 ,  -0.19959797,  35.737404  ,\n",
       "        -73.99057   ],\n",
       "       [  8.684312  ,  -0.9798555 ,  -0.19970776,  35.739094  ,\n",
       "        -73.97971   ],\n",
       "       ...,\n",
       "       [ 17.625313  ,  -0.66654694,  -0.745463  ,  37.62327   ,\n",
       "        -71.55693   ],\n",
       "       [ 17.644932  ,  -0.6665612 ,  -0.7454503 ,  37.624702  ,\n",
       "        -71.545746  ],\n",
       "       [ 17.653786  ,  -0.66656524,  -0.7454467 ,  37.626133  ,\n",
       "        -71.53456   ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "679c0b6c-575b-41c8-9a83-f5c492f6a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c\n",
    "# all_flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b7471b5-0b82-436c-a369-7945e148911d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 2: Fit the scaler on the global data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_flattened_data)  # Fit to the global data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc2e0629-9785-4b7d-8aa7-db997ef93fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Transform each flattened array using the fitted scaler\n",
    "scaled_data_list = []\n",
    "for data in flattened_data_list:\n",
    "    scaled_data = scaler.transform(data)  # Standardize using the global scaler\n",
    "    scaled_data_list.append(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dacfd221-3c45-450d-8f69-d4365160823a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42585, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78f00d34-d672-4984-bf6c-2bd34196f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Reshape scaled data back to (H, W, Channels)\n",
    "reshaped_scaled_data_list = []\n",
    "for idx, scaled_data in enumerate(scaled_data_list):\n",
    "    # Get original shape of the current array (speed, direction, lat, lon)\n",
    "    original_shape = all_arrays[idx][0].shape  # Assuming speed has the shape (H, W)\n",
    "    reshaped_data = scaled_data.reshape(original_shape[0], original_shape[1], 5)  # (H, W, 5)\n",
    "    reshaped_scaled_data_list.append(reshaped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "729adf0f-7831-43a9-93d7-1d6dca18d6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 255, 5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_scaled_data_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f9093e8-158f-4ce0-822f-4d4804e152ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose each image in reshaped_scaled_data_list\n",
    "reshaped_scaled_data_list_transposed = [np.transpose(image, (2, 0, 1)) for image in reshaped_scaled_data_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af4ccf66-3582-44ac-9a20-04dc67274d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 167, 255)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_scaled_data_list_transposed[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73782f51-5b66-42ed-b2bf-43d466508db1",
   "metadata": {},
   "source": [
    "# Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ee3e82e7-e81a-451e-9d9f-379c8b3fd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_list):\n",
    "        self.image_list = image_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image_list[idx]\n",
    "        image_tensor = torch.tensor(image, dtype=torch.float32)\n",
    "        return image_tensor\n",
    "\n",
    "\n",
    "import random \n",
    "# Now, let's split the dataset\n",
    "def split_dataset(images, train_ratio=0.7):\n",
    "    num_images = len(images)\n",
    "    indices = list(range(num_images))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    train_size = int(train_ratio * num_images)\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    return train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "46a085ac-34ef-4a29-bf4d-b7f2c9d74f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices = split_dataset(reshaped_scaled_data_list_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3e984045-4f92-4c6b-8274-af141a287b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 9, 1, 7, 5, 8, 3]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "931503bb-75ad-4389-952f-d8101fb68c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset with the transposed data\n",
    "dataset = CustomImageDataset(reshaped_scaled_data_list_transposed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6184a104-0df3-45ee-8af6-a1bf766954b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation datasets\n",
    "train_dataset = CustomImageDataset([reshaped_scaled_data_list_transposed[i] for i in train_indices])\n",
    "val_dataset = CustomImageDataset([reshaped_scaled_data_list_transposed[i] for i in val_indices])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e65407-2bcb-48fc-9d78-7f294cbfdd77",
   "metadata": {},
   "source": [
    "# now the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "832a9742-95ef-4b99-932b-b9a7751e149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Define parameters\n",
    "batch_size = 1  # You can adjust this as needed\n",
    "shuffle = True  # Shuffle the dataset for training\n",
    "\n",
    "# Create the data loader\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0c29ad28-8795-4f89-baf7-13f62d619143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 5, 167, 255]), Reconstructed shape: torch.Size([1, 5, 167, 255])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AdaptiveAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdaptiveAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(5, 32, kernel_size=3, stride=2, padding=1),  # Output: 32 x (H/2) x (W/2)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Output: 64 x (H/4) x (W/4)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # Output: 128 x (H/8) x (W/8)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # Output: 256 x (H/16) x (W/16)\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # Output: 128 x (H/8) x (W/8)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # Output: 64 x (H/4) x (W/4)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # Output: 32 x (H/2) x (W/2)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 5, kernel_size=4, stride=2, padding=1)    # Output: 5 x (H) x (W)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get input dimensions\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # Encode\n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        # Decode\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        # Resize the output to match the input dimensions\n",
    "        decoded = nn.functional.interpolate(decoded, size=(height, width), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "# Example of instantiation and testing\n",
    "autoencoder = AdaptiveAutoencoder()\n",
    "images = torch.randn(1, 5, 167, 255)  # Example input tensor\n",
    "output = model(images)\n",
    "print(f'Input shape: {images.shape}, Reconstructed shape: {output.shape}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "143606e3-5a44-46ef-8aa5-c986737bbf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (5, 167, 255)\n",
      "Shape (5, 167, 255)\n",
      "Shape (5, 150, 253)\n",
      "Shape (5, 194, 252)\n",
      "Shape (5, 167, 253)\n",
      "Shape (5, 169, 252)\n",
      "Shape (5, 193, 252)\n",
      "Shape (5, 193, 257)\n",
      "Shape (5, 194, 253)\n",
      "Shape (5, 193, 255)\n",
      "Shape (5, 167, 254)\n"
     ]
    }
   ],
   "source": [
    "for idx, image in enumerate(reshaped_scaled_data_list):\n",
    "    # Transpose the image to the desired shape (5, H, W)\n",
    "    image_input = np.transpose(image, (2, 0, 1))  # Shape: (5, H, W)\n",
    "    print(f\"Shape {image_input.shape}\")\n",
    "    # Convert to tensor without adding a batch dimension\n",
    "    image_tensor = torch.tensor(image_input, dtype=torch.float32)  # Shape: (5, H, W)\n",
    "    \n",
    "    # Add a batch dimension for the autoencoder (this is needed because the model expects input with a batch dimension)\n",
    "    image_tensor = image_tensor.unsqueeze(0)  # Shape: (1, 5, H, W)\n",
    "\n",
    "    # Forward pass through the autoencoder\n",
    "    # reconstructed_image = autoencoder(image_tensor)  # Shape will be (1, 5, H, W)\n",
    "\n",
    "    # # Compute loss (you may want to adjust how you handle loss calculation)\n",
    "    # loss = criterion(reconstructed_image, image_tensor)  # Compare with the original image tensor\n",
    "\n",
    "    # # Backward pass and optimization\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "69ba6460-c825-4ab4-b6a2-b9b6b45ab4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.8305\n",
      "Epoch [2/100], Loss: 0.9857\n",
      "Epoch [3/100], Loss: 0.5716\n",
      "Epoch [4/100], Loss: 1.4305\n",
      "Epoch [5/100], Loss: 0.4656\n",
      "Epoch [6/100], Loss: 1.2342\n",
      "Epoch [7/100], Loss: 0.8396\n",
      "Epoch [8/100], Loss: 0.8747\n",
      "Epoch [9/100], Loss: 1.4527\n",
      "Epoch [10/100], Loss: 0.4460\n",
      "Epoch [11/100], Loss: 0.4524\n",
      "Epoch [12/100], Loss: 0.6962\n",
      "Epoch [13/100], Loss: 1.4377\n",
      "Epoch [14/100], Loss: 0.5706\n",
      "Epoch [15/100], Loss: 0.4216\n",
      "Epoch [16/100], Loss: 0.5586\n",
      "Epoch [17/100], Loss: 0.7153\n",
      "Epoch [18/100], Loss: 0.4638\n",
      "Epoch [19/100], Loss: 0.4200\n",
      "Epoch [20/100], Loss: 0.5588\n",
      "Epoch [21/100], Loss: 1.0623\n",
      "Epoch [22/100], Loss: 0.9810\n",
      "Epoch [23/100], Loss: 0.5173\n",
      "Epoch [24/100], Loss: 0.3632\n",
      "Epoch [25/100], Loss: 0.5085\n",
      "Epoch [26/100], Loss: 0.3519\n",
      "Epoch [27/100], Loss: 0.4649\n",
      "Epoch [28/100], Loss: 0.2868\n",
      "Epoch [29/100], Loss: 0.2671\n",
      "Epoch [30/100], Loss: 0.5932\n",
      "Epoch [31/100], Loss: 0.3775\n",
      "Epoch [32/100], Loss: 0.4550\n",
      "Epoch [33/100], Loss: 0.5159\n",
      "Epoch [34/100], Loss: 0.2521\n",
      "Epoch [35/100], Loss: 0.4850\n",
      "Epoch [36/100], Loss: 0.3317\n",
      "Epoch [37/100], Loss: 0.7510\n",
      "Epoch [38/100], Loss: 0.7626\n",
      "Epoch [39/100], Loss: 0.6867\n",
      "Epoch [40/100], Loss: 0.4751\n",
      "Epoch [41/100], Loss: 0.6239\n",
      "Epoch [42/100], Loss: 0.6190\n",
      "Epoch [43/100], Loss: 0.2678\n",
      "Epoch [44/100], Loss: 0.3110\n",
      "Epoch [45/100], Loss: 0.2871\n",
      "Epoch [46/100], Loss: 0.4177\n",
      "Epoch [47/100], Loss: 0.4285\n",
      "Epoch [48/100], Loss: 0.3900\n",
      "Epoch [49/100], Loss: 0.2585\n",
      "Epoch [50/100], Loss: 0.5771\n",
      "Epoch [51/100], Loss: 0.3076\n",
      "Epoch [52/100], Loss: 0.6923\n",
      "Epoch [53/100], Loss: 0.3667\n",
      "Epoch [54/100], Loss: 0.4708\n",
      "Epoch [55/100], Loss: 0.5283\n",
      "Epoch [56/100], Loss: 0.2209\n",
      "Epoch [57/100], Loss: 0.4859\n",
      "Epoch [58/100], Loss: 0.2997\n",
      "Epoch [59/100], Loss: 0.2017\n",
      "Epoch [60/100], Loss: 0.4478\n",
      "Epoch [61/100], Loss: 0.4783\n",
      "Epoch [62/100], Loss: 0.2399\n",
      "Epoch [63/100], Loss: 0.5005\n",
      "Epoch [64/100], Loss: 0.2878\n",
      "Epoch [65/100], Loss: 0.4507\n",
      "Epoch [66/100], Loss: 0.2703\n",
      "Epoch [67/100], Loss: 0.3625\n",
      "Epoch [68/100], Loss: 0.2562\n",
      "Epoch [69/100], Loss: 0.2395\n",
      "Epoch [70/100], Loss: 0.3870\n",
      "Epoch [71/100], Loss: 0.4463\n",
      "Epoch [72/100], Loss: 0.1812\n",
      "Epoch [73/100], Loss: 0.1825\n",
      "Epoch [74/100], Loss: 0.2251\n",
      "Epoch [75/100], Loss: 0.4446\n",
      "Epoch [76/100], Loss: 0.2317\n",
      "Epoch [77/100], Loss: 0.2336\n",
      "Epoch [78/100], Loss: 0.3637\n",
      "Epoch [79/100], Loss: 0.3637\n",
      "Epoch [80/100], Loss: 0.3569\n",
      "Epoch [81/100], Loss: 0.1722\n",
      "Epoch [82/100], Loss: 0.2319\n",
      "Epoch [83/100], Loss: 0.3461\n",
      "Epoch [84/100], Loss: 0.3632\n",
      "Epoch [85/100], Loss: 0.3129\n",
      "Epoch [86/100], Loss: 0.3483\n",
      "Epoch [87/100], Loss: 0.2164\n",
      "Epoch [88/100], Loss: 0.3507\n",
      "Epoch [89/100], Loss: 0.3576\n",
      "Epoch [90/100], Loss: 0.3382\n",
      "Epoch [91/100], Loss: 0.2199\n",
      "Epoch [92/100], Loss: 0.3267\n",
      "Epoch [93/100], Loss: 0.2112\n",
      "Epoch [94/100], Loss: 0.4260\n",
      "Epoch [95/100], Loss: 0.3329\n",
      "Epoch [96/100], Loss: 0.2138\n",
      "Epoch [97/100], Loss: 0.3618\n",
      "Epoch [98/100], Loss: 0.2349\n",
      "Epoch [99/100], Loss: 0.4139\n",
      "Epoch [100/100], Loss: 0.3382\n"
     ]
    }
   ],
   "source": [
    "# Define your device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the autoencoder to the appropriate device\n",
    "autoencoder = AdaptiveAutoencoder().to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []  \n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for images in data_loader:\n",
    "        # Move the images to the appropriate device\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass: Pass images through the autoencoder\n",
    "        reconstructed_images = autoencoder(images)\n",
    "        # print(f'Input shape: {images.shape}, Reconstructed shape: {reconstructed_images.shape}')\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(reconstructed_images, images)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()        # Backpropagation\n",
    "        optimizer.step()       # Update weights\n",
    "\n",
    "    # Print loss for this epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6ffc4630-b944-426b-82e9-e0814d44e09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/400], Training Loss: 0.8821, Validation Loss: 0.9047\n",
      "Epoch [20/400], Training Loss: 0.6659, Validation Loss: 0.7946\n",
      "Epoch [30/400], Training Loss: 0.4879, Validation Loss: 0.6355\n",
      "Epoch [40/400], Training Loss: 0.3899, Validation Loss: 0.5186\n",
      "Epoch [50/400], Training Loss: 0.3388, Validation Loss: 0.5284\n",
      "Epoch [60/400], Training Loss: 0.3279, Validation Loss: 0.5529\n",
      "Epoch [70/400], Training Loss: 0.2861, Validation Loss: 0.5305\n",
      "Epoch [80/400], Training Loss: 0.2611, Validation Loss: 0.5241\n",
      "Epoch [90/400], Training Loss: 0.2412, Validation Loss: 0.5258\n",
      "Epoch [100/400], Training Loss: 0.2522, Validation Loss: 0.5150\n",
      "Epoch [110/400], Training Loss: 0.2261, Validation Loss: 0.5237\n",
      "Epoch [120/400], Training Loss: 0.2156, Validation Loss: 0.5278\n",
      "Epoch [130/400], Training Loss: 0.2259, Validation Loss: 0.5489\n",
      "Epoch [140/400], Training Loss: 0.1929, Validation Loss: 0.5328\n",
      "Epoch [150/400], Training Loss: 0.1902, Validation Loss: 0.5352\n",
      "Epoch [160/400], Training Loss: 0.2098, Validation Loss: 0.5911\n",
      "Epoch [170/400], Training Loss: 0.2363, Validation Loss: 0.4877\n",
      "Epoch [180/400], Training Loss: 0.1801, Validation Loss: 0.5249\n",
      "Epoch [190/400], Training Loss: 0.1790, Validation Loss: 0.5223\n",
      "Epoch [200/400], Training Loss: 0.1700, Validation Loss: 0.5308\n",
      "Epoch [210/400], Training Loss: 0.1669, Validation Loss: 0.5572\n",
      "Epoch [220/400], Training Loss: 0.1631, Validation Loss: 0.5040\n",
      "Epoch [230/400], Training Loss: 0.1607, Validation Loss: 0.5193\n",
      "Epoch [240/400], Training Loss: 0.1559, Validation Loss: 0.4869\n",
      "Epoch [250/400], Training Loss: 0.1455, Validation Loss: 0.5135\n",
      "Epoch [260/400], Training Loss: 0.1463, Validation Loss: 0.4603\n",
      "Epoch [270/400], Training Loss: 0.1149, Validation Loss: 0.4522\n",
      "Epoch [280/400], Training Loss: 0.1038, Validation Loss: 0.4553\n",
      "Epoch [290/400], Training Loss: 0.0877, Validation Loss: 0.4320\n",
      "Epoch [300/400], Training Loss: 0.0873, Validation Loss: 0.3944\n",
      "Epoch [310/400], Training Loss: 0.0903, Validation Loss: 0.3765\n",
      "Epoch [320/400], Training Loss: 0.0741, Validation Loss: 0.3718\n",
      "Epoch [330/400], Training Loss: 0.0766, Validation Loss: 0.3666\n",
      "Epoch [340/400], Training Loss: 0.0696, Validation Loss: 0.3518\n",
      "Epoch [350/400], Training Loss: 0.0669, Validation Loss: 0.3428\n",
      "Epoch [360/400], Training Loss: 0.0695, Validation Loss: 0.3345\n",
      "Epoch [370/400], Training Loss: 0.0643, Validation Loss: 0.3314\n",
      "Epoch [380/400], Training Loss: 0.0586, Validation Loss: 0.3165\n",
      "Epoch [390/400], Training Loss: 0.0693, Validation Loss: 0.3227\n",
      "Epoch [400/400], Training Loss: 0.0549, Validation Loss: 0.3178\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define your device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the autoencoder to the appropriate device\n",
    "autoencoder = AdaptiveAutoencoder().to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 400\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    autoencoder.train()  # Set the model to training mode\n",
    "    total_train_loss = 0\n",
    "    for images in train_loader:  # Use the training DataLoader\n",
    "        # Move the images to the appropriate device\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Forward pass: Pass images through the autoencoder\n",
    "        reconstructed_images = autoencoder(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(reconstructed_images, images)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()        # Backpropagation\n",
    "        optimizer.step()       # Update weights\n",
    "\n",
    "        total_train_loss += loss.item()  # Accumulate loss for the epoch\n",
    "\n",
    "    # Record average training loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        average_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(average_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        autoencoder.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for val_images in val_loader:  # Use the validation DataLoader\n",
    "                val_images = val_images.to(device)\n",
    "\n",
    "                # Forward pass: Pass images through the autoencoder\n",
    "                reconstructed_val_images = autoencoder(val_images)\n",
    "\n",
    "                # Compute the validation loss\n",
    "                val_loss = criterion(reconstructed_val_images, val_images)\n",
    "                total_val_loss += val_loss.item()  # Accumulate validation loss\n",
    "\n",
    "        average_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(average_val_loss)\n",
    "\n",
    "        # Print training and validation loss every 10 epochs\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {average_train_loss:.4f}, Validation Loss: {average_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bcc01c0e-d84a-41c8-86db-47a8e200c5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAHWCAYAAABACtmGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACeAklEQVR4nOzdd3xT1f/H8VeaLkops0BpKwUEGbIERJBSUJChCBSULyBLQUVQEHH9nLjX1y+Ie+ICUUBFRRGQUQGVIYKC7E3Zo0CBtun9/XFIS+lK2zTpeD8fj/tIcnPuuSc3oeSTc87n2CzLshAREREREZFs+Xi7ASIiIiIiIkWdAicREREREZFcKHASERERERHJhQInERERERGRXChwEhERERERyYUCJxERERERkVwocBIREREREcmFAicREREREZFcKHASERERERHJhQInESmShg4dSlRUVL6OffLJJ7HZbO5tUBGzY8cObDYbU6ZM8fi5bTYbTz75ZNrjKVOmYLPZ2LFjR67HRkVFMXToULe2pyCfFZGiwGazMXr0aG83Q0RyocBJRPLEZrO5tC1atMjbTS317rnnHmw2G1u2bMm2zCOPPILNZmPt2rUebFne7du3jyeffJI1a9Z4uylpnMHrK6+84u2meNyuXbu48847iYqKIiAggKpVq9KrVy+WLl3q7aZlKae/VXfeeae3mycixYSvtxsgIsXLp59+muHxJ598wrx58zLtb9CgQYHO895775GampqvYx999FEeeuihAp2/JBg4cCCTJ09m6tSpPP7441mWmTZtGo0bN6ZJkyb5Ps+gQYP4z3/+Q0BAQL7ryM2+ffuYMGECUVFRNGvWLMNzBfmsSN4tXbqU7t27AzB8+HAaNmzI/v37mTJlCtHR0UyaNIm7777by63MrHPnzgwePDjT/nr16nmhNSJSHClwEpE8ueWWWzI8/u2335g3b16m/RdLTEwkKCjI5fP4+fnlq30Avr6++Prqz1vr1q259NJLmTZtWpaB0/Lly9m+fTsvvPBCgc5jt9ux2+0FqqMgCvJZkbw5duwYffv2pUyZMixdupQ6deqkPTdu3Di6dOnC2LFjadGiBW3btvVYu86ePYu/vz8+PtkPpKlXr16uf6dERHKioXoi4nYdOnTg8ssvZ9WqVbRv356goCD+7//+D4Bvv/2W66+/nho1ahAQEECdOnV4+umncTgcGeq4eN7KhcOi3n33XerUqUNAQACtWrVixYoVGY7Nao6Tcw7BN998w+WXX05AQACNGjXip59+ytT+RYsW0bJlSwIDA6lTpw7vvPOOy/Om4uLiuOmmm7jkkksICAggMjKSe++9lzNnzmR6fcHBwezdu5devXoRHBxMaGgo48ePz3Qtjh8/ztChQylfvjwVKlRgyJAhHD9+PNe2gOl1+vfff1m9enWm56ZOnYrNZqN///4kJSXx+OOP06JFC8qXL0/ZsmWJjo5m4cKFuZ4jqzlOlmXxzDPPEBERQVBQEB07duSff/7JdOzRo0cZP348jRs3Jjg4mJCQELp168Zff/2VVmbRokW0atUKgGHDhqUNsXLO78pqjtPp06e57777iIyMJCAggMsuu4xXXnkFy7IylMvL5yK/Dh48yG233Ua1atUIDAykadOmfPzxx5nKffHFF7Ro0YJy5coREhJC48aNmTRpUtrzycnJTJgwgbp16xIYGEjlypVp164d8+bNy1DPv//+S9++falUqRKBgYG0bNmS2bNnZyjjal0Xe+edd9i/fz8vv/xyhqAJoEyZMnz88cfYbDaeeuopAFauXInNZsvy9c6dOxebzcb333+ftm/v3r3ceuutVKtWLe29+PDDDzMct2jRImw2G1988QWPPvoo4eHhBAUFkZCQkGPbXXHh3662bdtSpkwZatWqxdtvv52prKvva2pqKpMmTaJx48YEBgYSGhpK165dWblyZaayuX0OT548ydixYzMMkezcuXOW/75FxP30k6yIFIojR47QrVs3/vOf/3DLLbdQrVo1wHzJDg4OZty4cQQHB/PLL7/w+OOPk5CQwMsvv5xrvVOnTuXkyZPccccd2Gw2XnrpJWJjY9m2bVuuPQ+//vors2bN4q677qJcuXK89tpr9OnTh127dlG5cmUA/vzzT7p27UpYWBgTJkzA4XDw1FNPERoa6tLr/uqrr0hMTGTkyJFUrlyZP/74g8mTJ7Nnzx6++uqrDGUdDgddunShdevWvPLKK8yfP5///ve/1KlTh5EjRwImAOnZsye//vord955Jw0aNODrr79myJAhLrVn4MCBTJgwgalTp3LFFVdkOPeXX35JdHQ0l1xyCYcPH+b999+nf//+jBgxgpMnT/LBBx/QpUsX/vjjj0zD43Lz+OOP88wzz9C9e3e6d+/O6tWrue6660hKSspQbtu2bXzzzTfcdNNN1KpViwMHDvDOO+8QExPD+vXrqVGjBg0aNOCpp57i8ccf5/bbbyc6Ohog2x4Ny7K48cYbWbhwIbfddhvNmjVj7ty53H///ezdu5f//e9/Gcq78rnIrzNnztChQwe2bNnC6NGjqVWrFl999RVDhw7l+PHjjBkzBoB58+bRv39/rr32Wl588UUANmzYwNKlS9PKPPnkkzz//PMMHz6cK6+8koSEBFauXMnq1avp3LkzAP/88w9XX3014eHhPPTQQ5QtW5Yvv/ySXr16MXPmTHr37u1yXVn57rvvCAwM5Oabb87y+Vq1atGuXTt++eUXzpw5Q8uWLalduzZffvllps/s9OnTqVixIl26dAHgwIEDXHXVVWnBbGhoKD/++CO33XYbCQkJjB07NsPxTz/9NP7+/owfP55z587h7++f43tx9uxZDh8+nGl/SEhIhmOPHTtG9+7dufnmm+nfvz9ffvklI0eOxN/fn1tvvRVw/X0FuO2225gyZQrdunVj+PDhpKSkEBcXx2+//UbLli3TyrnyObzzzjuZMWMGo0ePpmHDhhw5coRff/2VDRs2ZPj3LSKFxBIRKYBRo0ZZF/8piYmJsQDr7bffzlQ+MTEx07477rjDCgoKss6ePZu2b8iQIVbNmjXTHm/fvt0CrMqVK1tHjx5N2//tt99agPXdd9+l7XviiScytQmw/P39rS1btqTt++uvvyzAmjx5ctq+Hj16WEFBQdbevXvT9m3evNny9fXNVGdWsnp9zz//vGWz2aydO3dmeH2A9dRTT2Uo27x5c6tFixZpj7/55hsLsF566aW0fSkpKVZ0dLQFWB999FGubWrVqpUVERFhORyOtH0//fSTBVjvvPNOWp3nzp3LcNyxY8esatWqWbfeemuG/YD1xBNPpD3+6KOPLMDavn27ZVmWdfDgQcvf39+6/vrrrdTU1LRy//d//2cB1pAhQ9L2nT17NkO7LMu81wEBARmuzYoVK7J9vRd/VpzX7JlnnslQrm/fvpbNZsvwGXD1c5EV52fy5ZdfzrbMxIkTLcD67LPP0vYlJSVZbdq0sYKDg62EhATLsixrzJgxVkhIiJWSkpJtXU2bNrWuv/76HNt07bXXWo0bN87wbyk1NdVq27atVbdu3TzVlZUKFSpYTZs2zbHMPffcYwHW2rVrLcuyrIcfftjy8/PL8O/23LlzVoUKFTJ8tm677TYrLCzMOnz4cIb6/vOf/1jly5dP+7e1cOFCC7Bq166d5b+3rADZbtOmTUsr5/zb9d///jdDW5s1a2ZVrVrVSkpKsizL9ff1l19+sQDrnnvuydSmC/9tuPo5LF++vDVq1CiXXrOIuJ+G6olIoQgICGDYsGGZ9pcpUybt/smTJzl8+DDR0dEkJiby77//5lpvv379qFixYtpjZ+/Dtm3bcj22U6dOGYYXNWnShJCQkLRjHQ4H8+fPp1evXtSoUSOt3KWXXkq3bt1yrR8yvr7Tp09z+PBh2rZti2VZ/Pnnn5nKX5zRKzo6OsNrmTNnDr6+vmk9UGDmFOVl8v0tt9zCnj17WLJkSdq+qVOn4u/vz0033ZRWp/NX99TUVI4ePUpKSgotW7bM8zCg+fPnk5SUxN13351heOPFPQZgPifOeSkOh4MjR44QHBzMZZddlu/hR3PmzMFut3PPPfdk2H/fffdhWRY//vhjhv25fS4KYs6cOVSvXp3+/fun7fPz8+Oee+7h1KlTLF68GIAKFSpw+vTpHIfKVahQgX/++YfNmzdn+fzRo0f55ZdfuPnmm9P+bR0+fJgjR47QpUsXNm/ezN69e12qKzsnT56kXLlyOZZxPu8cOtevXz+Sk5OZNWtWWpmff/6Z48eP069fP8D0Es6cOZMePXpgWVZa2w8fPkyXLl04ceJEps/DkCFDMvx7y03Pnj2ZN29epq1jx44Zyvn6+nLHHXekPfb39+eOO+7g4MGDrFq1CnD9fZ05cyY2m40nnngiU3suHvrryuewQoUK/P777+zbt8/l1y0i7qPASUQKRXh4eJZDZ/755x969+5N+fLlCQkJITQ0NG3C9okTJ3Kt95JLLsnw2BlEHTt2LM/HOo93Hnvw4EHOnDnDpZdemqlcVvuysmvXLoYOHUqlSpXS5i3FxMQAmV+fc75Ddu0B2LlzJ2FhYQQHB2cod9lll7nUHoD//Oc/2O12pk6dCpghS19//TXdunXLEIR+/PHHNGnSJG3OS2hoKD/88INL78uFdu7cCUDdunUz7A8NDc1wPjBB2v/+9z/q1q1LQEAAVapUITQ0lLVr1+b5vBeev0aNGpm+4DszPTrb55Tb56Igdu7cSd26dTMlLbi4LXfddRf16tWjW7duREREcOutt2aa3/LUU09x/Phx6tWrR+PGjbn//vszpJHfsmULlmXx2GOPERoammFzfnE/ePCgS3Vlp1y5cpw8eTLHMs7nnde/adOm1K9fn+nTp6eVmT59OlWqVOGaa64B4NChQxw/fpx33303U9udP8A42+5Uq1atXNt7oYiICDp16pRpcw4jdqpRowZly5bNsM+Zec85j8/V93Xr1q3UqFGDSpUq5do+Vz6HL730En///TeRkZFceeWVPPnkk24J8EXENQqcRKRQZPVL8PHjx4mJieGvv/7iqaee4rvvvmPevHlpczpcSSmdXfY266JJ/+4+1hUOh4POnTvzww8/8OCDD/LNN98wb968tCQGF78+T2Wic04gnzlzJsnJyXz33XecPHmSgQMHppX57LPPGDp0KHXq1OGDDz7gp59+Yt68eVxzzTWFmur7ueeeY9y4cbRv357PPvuMuXPnMm/ePBo1auSxFOOF/blwRdWqVVmzZg2zZ89Om5/VrVu3DPOC2rdvz9atW/nwww+5/PLLef/997niiit4//33gfTP1/jx47PsWZk3b17aDwC51ZWdBg0asHHjRs6dO5dtmbVr1+Ln55chcO7Xrx8LFy7k8OHDnDt3jtmzZ9OnT5+07JfOtt9yyy3Ztv3qq6/OcJ689DYVB658Dm+++Wa2bdvG5MmTqVGjBi+//DKNGjXK1IsqIoVDySFExGMWLVrEkSNHmDVrFu3bt0/bv337di+2Kl3VqlUJDAzMcsHYnBaRdVq3bh2bNm3i448/zrBeTG6ZynJSs2ZNFixYwKlTpzL0Om3cuDFP9QwcOJCffvqJH3/8kalTpxISEkKPHj3Snp8xYwa1a9dm1qxZGYYQZTXEyJU2A2zevJnatWun7T906FCmXpwZM2bQsWNHPvjggwz7jx8/TpUqVdIeu5LR8MLzz58/P9OwMudQUGf7PKFmzZqsXbuW1NTUDL0TWbXF39+fHj160KNHD1JTU7nrrrt45513eOyxx9ICnkqVKjFs2DCGDRvGqVOnaN++PU8++STDhw9Pu9Z+fn506tQp17blVFd2brjhBpYvX85XX32VZWrvHTt2EBcXR6dOnTIENv369WPChAnMnDmTatWqkZCQwH/+85+050NDQylXrhwOh8Olthemffv2cfr06Qy9Tps2bQJIy97o6vtap04d5s6dy9GjR13qdXJFWFgYd911F3fddRcHDx7kiiuu4Nlnn3V5OLGI5J96nETEY5y/qF74C2pSUhJvvvmmt5qUgd1up1OnTnzzzTcZ5hBs2bLFpV90s3p9lmVlSCmdV927dyclJYW33norbZ/D4WDy5Ml5qqdXr14EBQXx5ptv8uOPPxIbG0tgYGCObf/9999Zvnx5ntvcqVMn/Pz8mDx5cob6Jk6cmKms3W7P1LPz1Vdfpc3FcXJ+iXUlDXv37t1xOBy8/vrrGfb/73//w2azefQLZvfu3dm/f3+GYWopKSlMnjyZ4ODgtGGcR44cyXCcj49P2qLEzt6di8sEBwdz6aWXpj1ftWpVOnTowDvvvEN8fHymthw6dCjtfm51ZeeOO+6gatWq3H///ZmGiJ09e5Zhw4ZhWVamdcMaNGhA48aNmT59OtOnTycsLCzDjyd2u50+ffowc+ZM/v777xzbXthSUlJ455130h4nJSXxzjvvEBoaSosWLQDX39c+ffpgWRYTJkzIdJ689mg6HI5Mw1erVq1KjRo1cn3fRMQ91OMkIh7Ttm1bKlasyJAhQ7jnnnuw2Wx8+umnHh0SlZsnn3ySn3/+mauvvpqRI0emfQG//PLLWbNmTY7H1q9fnzp16jB+/Hj27t1LSEgIM2fOLNBcmR49enD11Vfz0EMPsWPHDho2bMisWbPyPP8nODiYXr16pc1zunCYHpiehFmzZtG7d2+uv/56tm/fzttvv03Dhg05depUns7lXI/q+eef54YbbqB79+78+eef/Pjjjxl6kZznfeqppxg2bBht27Zl3bp1fP755xl6qsD8cl+hQgXefvttypUrR9myZWndunWW81x69OhBx44deeSRR9ixYwdNmzbl559/5ttvv2Xs2LGZ1h8qqAULFnD27NlM+3v16sXtt9/OO++8w9ChQ1m1ahVRUVHMmDGDpUuXMnHixLQeseHDh3P06FGuueYaIiIi2LlzJ5MnT6ZZs2Zp82YaNmxIhw4daNGiBZUqVWLlypVpqamd3njjDdq1a0fjxo0ZMWIEtWvX5sCBAyxfvpw9e/akrY/lSl1ZqVy5MjNmzOD666/niiuuYPjw4TRs2JD9+/czZcoUtmzZwqRJk7JMFd+vXz8ef/xxAgMDue222zLND3rhhRdYuHAhrVu3ZsSIETRs2JCjR4+yevVq5s+fz9GjR/P2xlxk06ZNfPbZZ5n2V6tWLUMK9ho1avDiiy+yY8cO6tWrx/Tp01mzZg3vvvtu2pIHrr6vHTt2ZNCgQbz22mts3ryZrl27kpqaSlxcHB07dsz1el/o5MmTRERE0LdvX5o2bUpwcDDz589nxYoV/Pe//y3QtRERF3k4i5+IlDDZpSNv1KhRluWXLl1qXXXVVVaZMmWsGjVqWA888IA1d+5cC7AWLlyYVi67dORZpX7movTY2aUjzyqNb82aNTOkx7Ysy1qwYIHVvHlzy9/f36pTp471/vvvW/fdd58VGBiYzVVIt379eqtTp05WcHCwVaVKFWvEiBFpaYUvTKU9ZMgQq2zZspmOz6rtR44csQYNGmSFhIRY5cuXtwYNGmT9+eefLqcjd/rhhx8swAoLC8uUAjw1NdV67rnnrJo1a1oBAQFW8+bNre+//z7T+2BZuacjtyzLcjgc1oQJE6ywsDCrTJkyVocOHay///470/U+e/asdd9996WVu/rqq63ly5dbMTExVkxMTIbzfvvtt1bDhg3TUsM7X3tWbTx58qR17733WjVq1LD8/PysunXrWi+//HKGFNDO1+Lq5+Jizs9kdtunn35qWZZlHThwwBo2bJhVpUoVy9/f32rcuHGm923GjBnWddddZ1WtWtXy9/e3LrnkEuuOO+6w4uPj08o888wz1pVXXmlVqFDBKlOmjFW/fn3r2WefTUuR7bR161Zr8ODBVvXq1S0/Pz8rPDzcuuGGG6wZM2bkua6cXvuIESOsSy65xPLz87OqVKli3XjjjVZcXFy2x2zevDnt2vz6669Zljlw4IA1atQoKzIy0vLz87OqV69uXXvttda7776bVsaZjvyrr75yqa2WlXM68gs/Z86/XStXrrTatGljBQYGWjVr1rRef/31LNua2/tqWSbV/8svv2zVr1/f8vf3t0JDQ61u3bpZq1atytC+3D6H586ds+6//36radOmVrly5ayyZctaTZs2td58802Xr4OIFIzNsorQT70iIkVUr1698pW+WUSKjw4dOnD48OEshwuKiGiOk4jIRc6cOZPh8ebNm5kzZw4dOnTwToNERETE6zTHSUTkIrVr12bo0KHUrl2bnTt38tZbb+Hv788DDzzg7aaJiIiIlyhwEhG5SNeuXZk2bRr79+8nICCANm3a8Nxzz2Va0FVERERKD81xEhERERERyYXmOImIiIiIiORCgZOIiIiIiEguSt0cp9TUVPbt20e5cuWw2Wzebo6IiIiIiHiJZVmcPHmSGjVqZFqY+2KlLnDat28fkZGR3m6GiIiIiIgUEbt37yYiIiLHMqUucCpXrhxgLk5ISEi+6khOTubnn3/muuuuw8/Pz53Nk4voWnuGrrNn6Dp7jq61Z+g6e4aus+foWntGUbrOCQkJREZGpsUIOSl1gZNzeF5ISEiBAqegoCBCQkK8/maXdLrWnqHr7Bm6zp6ja+0Zus6eoevsObrWnlEUr7MrU3iUHEJERERERCQXCpxERERERERyocBJREREREQkF6VujpOIiIiIFD2WZZGSkoLD4fBaG5KTk/H19eXs2bNebUdJ5+nr7Ofnh91uL3A9CpxERERExKuSkpKIj48nMTHRq+2wLIvq1auze/durfdZiDx9nW02GxEREQQHBxeoHgVOIiIiIuI1qampbN++HbvdTo0aNfD39/da0JKamsqpU6cIDg7OdTFUyT9PXmfLsjh06BB79uyhbt26Bep5UuAkIiIiIl6TlJREamoqkZGRBAUFebUtqampJCUlERgYqMCpEHn6OoeGhrJjxw6Sk5MLFDjpEyEiIiIiXqdARQqLu3ow9QkVERERERHJhYbqeVOqAw7FwZl4KBMGodHgU/CMHyIiIiIi4l7qcfKW3bNgdhQs6AjLBpjb2VFmv4iIiIjkicMBixbBtGnmtjhmE4+KimLixIkul1+0aBE2m43jx48XWpsknQInb9g9C+L6QuKejPsT95r9Cp5EREREXDZrFkRFQceOMGCAuY2KMvsLg81my3F78skn81XvihUruP32210u37ZtW+Lj4ylfvny+zucqBWiGhup5WqoDVo0BrCyetAAbrBoL4T01bE9EREQkF7NmQd++YF301WrvXrN/xgyIjXXvOePj49PuT58+nccff5yNGzem7btwvSDLsnA4HPj65v61OzQ0NE/t8Pf3p3r16nk6RvJPPU6ediguc09TBhYk7jblREREREoZy4LTp13bEhLgnnsyB03OegDGjDHlXKkvq3qyUr169bStfPny2Gy2tMf//vsv5cqV48cff6RFixYEBATw66+/snXrVnr27Em1atUIDg6mVatWzJ8/P0O9Fw/Vs9lsvP/++/Tu3ZugoCDq1q3L7Nmz056/uCdoypQpVKhQgblz59KgQQOCg4Pp2rVrhkAvJSWFe+65hwoVKlC5cmUefPBBhgwZQq9evVx78Vk4duwYgwcPpmLFigQFBdGtWzc2b96c9vzOnTvp0aMHFStWpGzZsjRu3Jiff/457diBAwcSGhpKmTJlqFu3Lh999FG+21KYFDh52pn43MvkpZyIiIhICZKYCMHBrm3ly5uepexYFuzZY8q5Ul9iovtex0MPPcQLL7zAhg0baNKkCadOnaJ79+4sWLCAP//8k65du9KjRw927dqVYz0TJkzg5ptvZu3atXTv3p2BAwdy9OjRbMsnJibyyiuv8Omnn7JkyRJ27drF+PHj055/8cUX+fzzz/noo49YunQpCQkJfPPNNwV6rUOHDmXlypXMnj2b5cuXY1kW3bt3Jzk5GYBRo0Zx7tw5lixZwrp163j++ecpW7YsAI899hjr16/nxx9/ZMOGDbz11ltUqVKlQO0pLBqq52llwtxbTkRERESKnKeeeorOnTunPa5UqRJNmzZNe/z000/z9ddfM3v2bEaPHp1tPUOHDqV///4APPfcc7z22mv88ccfdO3aNcvyycnJvP3229SpUweA0aNH89RTT6U9P3nyZB5++GF69+4NwOuvv86cOXPy/To3b97M7NmzWbp0KW3btgXg888/JzIykm+++YabbrqJXbt20adPHxo3bgyYnrWEhAQAdu3aRfPmzWnZsmXac0WVepw8LTQagiKA7BbiskFQpCknIiIiUsoEBcGpU65trn7fnzPHtfqCgtz3OpyBgNOpU6cYP348DRo0oEKFCgQHB7Nhw4Zce5yaNGmSdr9s2bKEhIRw8ODBbMsHBQWlBU0AYWFhaeVPnDjBgQMHuPLKK9Oet9vttGjRIk+v7UIbNmzA19eX1q1bp+2rXLkyl112GRs2bADgnnvu4ZlnnuHqq6/miSeeYO3atWllR44cyRdffEGzZs144IEHWLZsWb7bUtgUOHmajx1aTDr/4OLg6fzjFhOVGEJERERKJZsNypZ1bbvuOoiIMMdkV1dkpCnnSn3Z1ZMfzqFoTuPHj+frr7/mueeeIy4ujjVr1tC4cWOSkpJyrMfPz++i12QjNTU1T+UtVydvFZLhw4ezbds2Bg0axLp167jyyit59913AejWrRs7d+7k3nvvZd++fVx77bUZhhYWJQqcvCEyFqJnQFB4xv3+lcz+SDenfhEREREpgex2mHT+9+iLgx7n44kTTTlvW7p0KUOHDqV37940btyY6tWrs2PHDo+2oXz58lSrVo0VK1ak7XM4HKxevTrfdTZo0ICUlBR+//33tH1Hjhxh48aNNGzYMG1fZGQkd955J7NmzWLcuHF8/PHHac+FhoYyZMgQPvvsMyZOnJgWVBU1muPkLZGxOKr3ZN0vcVQ+MolIvsGqeAU2BU0iIiIiLouNNSnHx4wxiSCcIiJM0OTuVOT5VbduXWbNmkWPHj2w2Ww89thjOfYcFZa7776b559/nksvvZT69eszefJkjh07hs2F7rZ169ZRrly5tMc2m42mTZvSs2dPRowYwTvvvEO5cuV46KGHCA8Pp2fPngCMHTuWbt26Ua9ePY4dO8aiRYu47LLLAHj88cdp0aIFjRo14ty5c3z//fc0aNCgcF58ASlw8pJZs2DMGDt79nQgKrQmW1/9Fp8D8/hpxna69q3l7eaJiIiIFBuxsdCzJ8TFQXw8hIVBdHTR6GlyevXVV7n11ltp27YtVapU4cEHH0xLkOBJDz74IPv372fw4MHY7XZuv/12unTpgt2Fi9W+ffsMj+12OykpKXz00UeMGTOGG264gaSkJNq3b8+cOXPShg06HA5GjRrFnj17CAkJoUuXLkyYMAEwa1E9/PDD7NixgzJlyhAdHc0XX3zh/hfuBjbL24MePSwhIYHy5ctz4sQJQkJC8lVHcnIyc+bMoXv37pnGkboiq4XafnqwC12a/Mxz3/4f9fs9W2R+HfG2gl5rcY2us2foOnuOrrVn6Dp7Rkm/zmfPnmX79u3UqlWLwMBAr7YlNTWVhIQEQkJC8PEpHTNaUlNTadCgATfffDNPP/20x87pyeuc02csL7FB6fhEFCEOh+lKvjhcffeX2wEYFvMh48cl43B4oXEiIiIiUqLt3LmT9957j02bNrFu3TpGjhzJ9u3bGTBggLebVuQpcPKwuLiM42+dZq++kf3HqxFWYT9NQ78nLs7zbRMRERGRks3Hx4cpU6bQqlUrrr76atatW8f8+fOL7LyiokRznDwsPj7r/SkOPz5aMoyHb3yB2695l/j43p5tmIiIiIiUeJGRkSxdutTbzSiW1OPkYWFh2T/3/sLhAHRpPJdaoTs80yAREREREcmVAicPi47OfqG2bQfrMG9dJ3x8LK6s8oHnGyciIiIiIllS4ORhuS3U9t5CkyTCZ/sHkJri4daJiIiIiEhWFDh5gXOhtvDwjPsjIqD//T0hIBTOxMO+H7zTQBERERERyUCBk5fExsKOHfB//2ceN2wI27dD7z7+UHuY2bnlPa+1T0RERERE0ilw8iK7HQYPNve3bbtgbac6JkkE8T/C6V1eaZuIiIiIiKRT4ORldetCuXJw9iysX39+Z0hdqHYNWKmw9UOvtk9ERESkWEh1wIFFsGOauU11eLtFuerQoQNjx45NexwVFcXEiRNzPMZms/HNN98U+Nzuqqc0UeDkZT4+0KKFub9y5QVPXGqSRLBNSSJEREREcrR7FsyOggUdYdkAczs7yuwvBD169KBr165ZPhcXF4fNZmPt2rV5rnfFihXcfvvtBW1eBk8++STNmjXLtD8+Pp5u3bq59VwXmzJlChUqVCjUc3iSAqcioGVLc5shcIroBQGVIXEPxP/kjWaJiIiIFH27Z0FcX/Od6UKJe83+QgiebrvtNubNm8eePXsyPffRRx/RsmVLmjRpkud6Q0NDCQoKckcTc1W9enUCAgI8cq6SQoFTEZBl4GQPgFpDzf0t73q6SSIiIiLeYVmQctq1LSkBVt4DWFlVZG5WjjHlXKnPyqqezG644QZCQ0OZMmVKhv2nTp3iq6++4rbbbuPIkSP079+f8PBwgoKCaNy4MdOmTcux3ouH6m3evJn27dsTGBhIw4YNmTdvXqZjHnzwQerVq0dQUBC1a9fmscceIzk5GTA9PhMmTOCvv/7CZrNhs9nS2nzxUL1169ZxzTXXUKZMGSpXrsztt9/OqVOn0p4fOnQovXr14pVXXiEsLIzKlSszatSotHPlx65du+jZsyfBwcGEhIRw8803c+DAgbTn//rrLzp27Ei5cuUICQmhRYsWrDz/hXnnzp306NGDihUrUrZsWRo1asScOXPy3RZX+BZq7eISZ+C0di0kJYG///knLh0B//7XpCVP3ANBEV5ro4iIiIhHOBLhy2A3VWbBmT0wo7xrxfsmuFTM19eXwYMHM2XKFB555BFs5xfn/Oqrr3A4HPTv359Tp07RokULHnzwQUJCQvjhhx8YNGgQderU4corr8z1HKmpqcTGxlKtWjV+//13Tpw4kWE+lFO5cuWYMmUKNWrUYN26dYwYMYJy5crxwAMP0K9fP/7++29++ukn5s+fD0D58pmvxenTp+nSpQtt2rRhxYoVHDx4kOHDhzN69OgMweHChQsJCwtj4cKFbNmyhX79+tGsWTNGjBjh0nW7+PX17t2b4OBgFi9eTEpKCqNGjaJfv34sWrQIgIEDB9K8eXPeeust7HY7a9aswc/PD4BRo0aRlJTEkiVLKFu2LOvXryc42F2fm6wpcCoCateGChXg+HH45x9o3vz8EyGXQdUYOLjYJIlo/LgXWykiIiIiTrfeeisvv/wyixcvpkOHDoAZptenTx/Kly9P+fLlGT9+fFr5u+++m7lz5/Lll1+6FDjNnz+ff//9l7lz51KjRg0AnnvuuUzzkh599NG0+1FRUYwfP54vvviCBx54gDJlyhAcHIyvry/Vq1fP9lxTp07l7NmzfPLJJ5QtWxaA119/nR49evDiiy9SrVo1ACpWrMjrr7+O3W6nfv36XH/99SxYsCBfgdPixYtZt24d27dvJzIyEoBPPvmERo0asWLFClq1asWuXbu4//77qV+/PgB169ZNO37Xrl306dOHxo0bA1C7du08tyGvFDgVATab6XWaP98M10sLnMAkiTi4GLa+D40eAR+719opIiIiUujsQXDzqdzLARxcAou6516uwxyo2j73crZA4KRLp65fvz5t27blww8/pEOHDmzZsoW4uDieeuopABwOB8899xxffvkle/fuJSkpiXPnzrk8h2nDhg1ERkamBU0Abdq0yVRu+vTpvPbaa2zdupVTp06RkpJCSEiIS+e48FxNmzZNC5oArr76alJTU9m4cWNa4NSoUSPs9vTvomFhYaxbty5P53LatGkTkZGRaUETQMOGDalQoQIbNmygVatWjBs3juHDh/Ppp5/SqVMnbrrpJurUqQPAPffcw8iRI/n555/p1KkTffr0yde8srzQHKciIst5TgCRseBfCRJ3Q/xcj7dLRERExKNsNvAt69pW/brzUxls2VUGQZGmnCv12bKrJ2u33XYbM2fO5OTJk3z00UfUqVOHmJgYAF5++WUmTZrEgw8+yMKFC1mzZg1dunQhKSmpYNfnAsuXL2fgwIF0796d77//nj///JNHHnnEree4kHOYnJPNZiM1NbVQzgUmI+A///zD9ddfzy+//ELDhg35+uuvARg+fDjbtm1j0KBBrFu3jpYtWzJ58uRCawsocCoysg2c7IFQa4i5v1VJIkRERETS+NihxaTzDy4Oes4/bjGx0Ebs3Hzzzfj4+DB16lQ++eQTbr311rT5TkuXLqVnz57ccsstNG3alNq1a7Np0yaX627QoAG7d+8mPj4+bd9vv/2WocyyZcuoWbMmjzzyCC1btqRu3brs3LkzQxl/f38cjpzXtGrQoAF//fUXp0+fTtu3dOlSfHx8uOyyy1xuc17Uq1eP3bt3s3v37rR969ev5/jx4zRs2DBDuXvvvZeff/6Z2NhYPvroo7TnIiMjufPOO5k1axb33Xcf7733XqG01UmBUxHhDJzWrTOL4WZw6flxo3u/N6k1RURERMSIjIXoGRAUnnF/UITZHxlbaKcODg6mX79+PPzww8THxzN06NC05+rWrcu8efNYtmwZGzZs4I477siQMS43nTp1ol69egwZMoS//vqLuLg4HnnkkQxl6taty65du/jiiy/YunUrr732WlqPjFNUVBTbt29nzZo1HD58mHPnzmU618CBAwkMDGTIkCH8/fffLFy4kLvvvptBgwalDdPLL4fDwZo1azJsGzZsoEOHDjRu3JiBAweyevVq/vjjDwYPHkxMTAwtW7bkzJkzjB49mkWLFrFz506WLl3KihUraNCgAQBjx45l7ty5bN++ndWrV7Nw4cK05wqLAqci4pJLoEoVSE42wVMG5RtAaDRYDtj2UZbHi4iIiJRakbFw4w64diG0nWpub9xeqEGT02233caxY8fo0qVLhvlIjz76KFdccQVdunShQ4cOVK9enV69erlcr4+PD19//TVnzpzhyiuvZPjw4Tz77LMZytx4443ce++9jB49mmbNmrFs2TIee+yxDGX69OlD165d6dixI6GhoVmmRA8KCmLu3LkcPXqUVq1a0bdvX6699lpef/31vF2MLJw6dYrmzZtn2Hr27InNZuPrr7+mYsWKtG/fnk6dOlG7dm2mT58OgN1u58iRIwwePJh69epx8803061bNyZMmACYgGzUqFE0aNCArl27Uq9ePd58880CtzcnNstyMWF9CZGQkED58uU5ceJEnifOOSUnJzNnzhy6d++eaaxnQXTrBj/9BG++CSNHXvTk9s9g+SAoWxNu3Aa20hHzFta1lox0nT1D19lzdK09Q9fZM0r6dT579izbt2+nVq1aBAYGerUtqampJCQkEBISgo9P6fiu5Q2evs45fcbyEhvoE1GEZDvPCSCyD/hXhNM7IT7z4mciIiIiIlJ4FDgVITkGTr5loNZgc19JIkREREREPEqBUxHiDJz++QcSE7MoUOd8kog9s+FMfBYFRERERESkMChwKkJq1IBq1cDhgL/+yqJAhUZQpS1YKbBtiqebJyIiIiJSailwKkJstlyG6wFceru53fIeWIW34JiIiIiIJ5WyfGXiQe76bClwKmJyDZwuuQn8ysPp7bB/gcfaJSIiIlIYnJkCE7OcpyBScElJSYBJcV4Qvu5ojLhProGTbxDUGgSbXoct70JYZ4+1TURERMTd7HY7FSpU4ODBg4BZU8hms3mlLampqSQlJXH27FmlIy9EnrzOqampHDp0iKCgIHx9Cxb6KHAqYlq0MLcbNsCpUxAcnEWhS283gdOeb+DMAShTsBWdRURERLypevXqAGnBk7dYlsWZM2coU6aM14K30sDT19nHx4dLLrmkwOdS4FTEhIVBeDjs3Qt//gnR0VkUqtAYKl8FR36D7VOg4YOebqaIiIiI29hsNsLCwqhatSrJyclea0dycjJLliyhffv2JXKx4aLC09fZ39/fLT1bCpyKoJYtTeC0alU2gROYXqcjv5kkEQ3uB5u6k0VERKR4s9vtBZ6HUtDzp6SkEBgYqMCpEBXX6+z1b9tvvPEGUVFRBAYG0rp1a/74448cy0+cOJHLLruMMmXKEBkZyb333svZs2c91FrPyHWeE0DNm8EvBE5thQMLPdIuEREREZHSyquB0/Tp0xk3bhxPPPEEq1evpmnTpnTp0iXb8a1Tp07loYce4oknnmDDhg188MEHTJ8+nf/7v//zcMsLl0uBk29ZiLrF3N/ybqG3SURERESkNPNq4PTqq68yYsQIhg0bRsOGDXn77bcJCgriww8/zLL8smXLuPrqqxkwYABRUVFcd9119O/fP9dequLGmSBi40ZISMihoHNNp92zYPc3sGMaHFgEqY5CbqGIiIiISOnitTlOSUlJrFq1iocffjhtn4+PD506dWL58uVZHtO2bVs+++wz/vjjD6688kq2bdvGnDlzGDRoULbnOXfuHOfOnUt7nHA+EklOTs735EPncYU1ebFCBahZ05edO2388UcKMTHZLNoV3BB7cB18Tm2FuN5pu60y4TiavYoV0Tvr44qRwr7WYug6e4aus+foWnuGrrNn6Dp7jq61ZxSl65yXNtgsLy3TvG/fPsLDw1m2bBlt2rRJ2//AAw+wePFifv/99yyPe+211xg/fjyWZZGSksKdd97JW2+9le15nnzySSZMmJBp/9SpUwkKCir4CykkL77YiuXLazB06N/06rU1yzJhKctpde5FLk6s6HxDVwQ8SLxvm4sPExERERERzMLLAwYM4MSJE4SEhORYtlhl1Vu0aBHPPfccb775Jq1bt2bLli2MGTOGp59+msceeyzLYx5++GHGjRuX9jghIYHIyEiuu+66XC9OdpKTk5k3bx6dO3cutEwgf//tw/LlcPp0Q7p3vyxzAcuB7w+jsjzWBljYaOXzOSndngSb97LTFJQnrrXoOnuKrrPn6Fp7hq6zZ+g6e46utWcUpeuckOO8mIy8FjhVqVIFu93OgQMHMuw/cOBA2iJoF3vssccYNGgQw4cPB6Bx48acPn2a22+/nUceeSTL/OwBAQEEBARk2u/n51fgN8oddWSndWtzu3q1D35+WUxFO7AUzuzN9ngbFpzZg9+x36Bah0JpoycV5rWWdLrOnqHr7Dm61p6h6+wZus6eo2vtGUXhOufl/F5LDuHv70+LFi1YsGBB2r7U1FQWLFiQYejehRITEzMFR85c/14acVhonAkitm6FY8eyKHAm3rWKXC0nIiIiIiLZ8mpWvXHjxvHee+/x8ccfs2HDBkaOHMnp06cZNmwYAIMHD86QPKJHjx689dZbfPHFF2zfvp158+bx2GOP0aNHD68ullYYKlaEOnXM/VWrsihQJsy1ilwtJyIiIiIi2fLqHKd+/fpx6NAhHn/8cfbv30+zZs346aefqFatGgC7du3K0MP06KOPYrPZePTRR9m7dy+hoaH06NGDZ5991lsvoVC1bGl6nFauhE6dLnoyNBqCIiBxL+npIC5kM8+HRnugpSIiIiIiJZvXk0OMHj2a0aNHZ/ncokWLMjz29fXliSee4IknnvBAy7yvZUuYPj2bhXB97NBiEsT1xZkOIt35PHstJppyIiIiIiJSIF4dqic5a9nS3GYZOAFExkL0DAgKz7i/TLjZHxlbqO0TERERESktFDgVYc2bm9udO+HQoWwKRcbCjTvgml/Av5LZd+U7CppERERERNxIgVMRVr481Ktn7meZIMLJxw7VO0Jkb/P4wIIcCouIiIiISF4pcCrich2ud6Hq15nb+LmF1h4RERERkdJIgVMR5wyccuxxcqreCWw+cOIfSNxTqO0SERERESlNFDgVcXnqcQqoBJVamfvx8wqtTSIiIiIipY0CpyKueXOw2WDPHti/34UDwjRcT0RERETE3RQ4FXHBwdCggbnv0nC9sC7mdv88SHUUWrtEREREREoTBU7FQJ6G61W+EvxCIOkoHFtdqO0SERERESktFDgVA3kKnHz8oNq15r6G64mIiIiIuIUCp2LgwsDJslw4wDlcL/7nQmuTiIiIiEhposCpGGjaFOx2kxxi3z4XDnAmiDi8HJITCrVtIiIiIiKlgQKnYiAoCBo1MvddGq4XXAvK1QUrBfb/UqhtExEREREpDRQ4FRN5mucEUP18r9N+DdcTERERESkoBU7FRJ4Dp7R5TkoQISIiIiJSUAqciok8J4io1tFk2Du1DU5uLdS2iYiIiIiUdAqciokmTcDPDw4fhl27XDjALxiqtDX31eskIiIiIlIgCpyKiYAAaNzY3NdwPRERERERz1LgVIzkfZ7T+QQRB36B1ORCaZOIiIiISGmgwKkYyXPgVLE5BIRCyimzppOIiIiIiOSLAqdipEULc7tqlYsJImw+UL2zuR+vtOQiIiIiIvmlwKkYufxy8PeHY8dg+3YXD3IO19M8JxERERGRfFPgVIz4+0PTpuZ+nuc5HV0FZw8XSrtEREREREo6BU7FTJ7nOZUJgwpNAAv2zy+sZomIiIiIlGgKnIqZPAdOkN7rtF/D9URERERE8kOBUzHjDJxWrYLUVBcPSlvP6WcXs0qIiIiIiMiFFDgVMw0bQmAgJCTAli0uHhTaDuxl4Mw+OPFPobZPRERERKQkUuBUzPj6QvPm5r7Lw/XsgVA1xtxXdj0RERERkTxT4FQM5W+e0wXD9UREREREJE8UOBVDBUoQcWgJpJxxe5tEREREREoyBU7FkDNwWr0aHA4XDwppAEER4DgLh+IKrW0iIiIiIiWRAqdi6LLLoGxZOH0aNm508SCbDaqf73XSPCcRERERkTxR4FQM2e1wxRXmfv7mOSlwEhERERHJCwVOxVS+5jlV7wTYTEryxL2F0SwRERERkRJJgVMxla/AKaASVG5l7iu7noiIiIiIyxQ4FVPOwOnPPyElJQ8HOofr7VfgJCIiIiLiKgVOxdSll0JICJw9C+vX5+FAZ4KI/fMg1dWUfCIiIiIipZsCp2LKxwdatDD3V63Kw4FVWoNfCJw7AsdWF0rbRERERERKGgVOxVi+5jn5+EG1a819zXMSEREREXGJAqdiLF+BE0CY1nMSEREREckLBU7FmHOo3l9/QVJSHg50Jog4vBySE9zeLhERERGRkkaBUzFWuzZUqADnzsE//+ThwOBaEHwpWClwYGFhNU9EREREpMRQ4FSM2WwFGa53vtdJw/VERERERHKlwKmYK3jgpAQRIiIiIiK5UeBUzDkDp4ULYdo0WLQIHK4sz1StA9h84dRWOLm1EFsoIiIiIlL8KXAq5g4fNrebN8OAAdCxI0RFwaxZuRzoVw5Crzb3NVxPRERERCRHCpyKsVmzYOTIzPv37oW+fV0InpxpyfdruJ6IiIiISE4UOBVTDgeMGQOWlfk5576xY3MZtuec57T/F0hNdncTRURERERKDAVOxVRcHOzZk/3zlgW7d5ty2arYHAKqQMpJOPyb29soIiIiIlJSKHAqpuLj3VDO5gPVO58vqHlOIiIiIiLZUeBUTIWFuamc1nMSEREREcmVAqdiKjoaIiLMIrhZsdkgMtKUy5Gzx+noKjh72K1tFBEREREpKRQ4FVN2O0yaZO5fHDw5H0+caMrlKKgGVGgMWLB/vptbKSIiIiJSMihwKsZiY2HGDAgPz7g/IsLsj411saK07HpKSy4iIiIikhUFTsVcbCzs2AHffZfe07RsWR6CJoDq59dzip+bdX5zEREREZFSToFTCWC3ww03QPPm5vGvv+axgqrRYA+EM/vgxD9ub5+IiIiISHGnwKkEiYkxt4sX5/FAeyBUPX/wpjdhxzQ4sAhSc1o9V0RERESk9FDgVII4A6dFi/JxcOD5vOVb3oJlA2BBR5gdBbtnual1IiIiIiLFlwKnEiQ62sxz+vdfOHAgDwfungXbP868P3EvxPVV8CQiIiIipZ4CpxKkUiVo3NjcX7LExYNSHbBqDJBVUojz+1aN1bA9ERERESnVFDiVMHme53QoDhL35FDAgsTdppyIiIiISCmlwKmEyXPgdCbeveVEREREREogBU4lTPv25vbvv+HwYRcOKBPmWsWulhMRERERKYEUOJUwoaHQsKG5H+fK6LrQaAiKAGzZFLBBUKQpJyIiIiJSSilwKoHyNFzPxw4tJp1/cHHwdP5xi4mmnIiIiIhIKaXAqQTK8zynyFiIngFB4Rn3+1cw+yNj3dk8EREREZFiR4FTCeQMnP76C44dc/GgyFi4cQdcuxBqDjD7yl2moElEREREBAVOJVL16nDZZWBZ8OuveTjQxw7VOsAVr4DNDkd+g4SNhdVMEREREZFiQ4FTCZXn4XoXKhMGYV3N/W0fu61NIiIiIiLFlQKnEqpAgRNA7aHmdvsnkOpwR5NERERERIotBU4llDNwWr0aTpzIRwXhPcC/IpzZCwcWuLVtIiIiIiLFjQKnEio8HOrUgdRUWLo0HxXYA9KTRGyb4s6miYiIiIgUO14PnN544w2ioqIIDAykdevW/PHHHzmWP378OKNGjSIsLIyAgADq1avHnDlzPNTa4sVtw/X2fA1Jx93QIhERERGR4smrgdP06dMZN24cTzzxBKtXr6Zp06Z06dKFgwcPZlk+KSmJzp07s2PHDmbMmMHGjRt57733CA8Pz7J8aVfgwKlSCyjfCBxnYdeXbmuXiIiIiEhx49XA6dVXX2XEiBEMGzaMhg0b8vbbbxMUFMSHH36YZfkPP/yQo0eP8s0333D11VcTFRVFTEwMTZs29XDLiwdn4LRyJZw6lY8KbLb0XicN1xMpGlIdcGAR7JhmbpW8RURExCN8vXXipKQkVq1axcMPP5y2z8fHh06dOrF8+fIsj5k9ezZt2rRh1KhRfPvtt4SGhjJgwAAefPBB7HZ7lsecO3eOc+fOpT1OSEgAIDk5meTk5Hy13Xlcfo/3lBo1oGZNX3butLFkSQqdO1t5ryT8ZnzXPITt8HKSj/5tFsX1oOJyrYs7XWfPKOh1tu35GvuacdjO7E3bZ5UJx9HsVayI3m5pY0mhz7Rn6Dp7hq6z5+hae0ZRus55aYPNsqx8fJsuuH379hEeHs6yZcto06ZN2v4HHniAxYsX8/vvv2c6pn79+uzYsYOBAwdy1113sWXLFu666y7uuecennjiiSzP8+STTzJhwoRM+6dOnUpQUJD7XlARNWlScxYuvIS+fTdxyy0b8lVH67PPUN2xkk1+fdjgP8jNLRQRV4SlLKfVuRcBsF2w3/kHfEXAg8T7tsl0nIiIiGQvMTGRAQMGcOLECUJCQnIs67Uep/xITU2latWqvPvuu9jtdlq0aMHevXt5+eWXsw2cHn74YcaNG5f2OCEhgcjISK677rpcL052kpOTmTdvHp07d8bPzy9fdXjKwYM2Fi6EffsupXv3Wvmqw7bnDCzvT137cmp1+wxsWffuFYbidK2LM11nz8j3dbYc+P4wCsgYNDkfW9ho5fM5Kd2e9Oi/z6JMn2nP0HX2DF1nz9G19oyidJ2do9Fc4bXAqUqVKtjtdg4cOJBh/4EDB6hevXqWx4SFheHn55dhWF6DBg3Yv38/SUlJ+Pv7ZzomICCAgICATPv9/PwK/Ea5o47Cds015nbFCh+Sk33IVyfbJb1hVSVsZ/fhd3gx1Oji1ja6ojhc65JA19kz8nydDyw1a6plw4YFZ/bgd+w3qNah4A0sQfSZ9gxdZ8/QdfYcXWvPKArXOS/n91pyCH9/f1q0aMGCBemLq6amprJgwYIMQ/cudPXVV7NlyxZSU1PT9m3atImwsLAsgyaB2rXNmk7JyfDbb/msxB4AUefXdNo+xV1NExFXnYl3bzkRERHJM69m1Rs3bhzvvfceH3/8MRs2bGDkyJGcPn2aYcOGATB48OAMySNGjhzJ0aNHGTNmDJs2beKHH37gueeeY9SoUd56CUWezQYdOpj7+U5LDunZ9XZrTScRjysT5t5yIiIikmdenePUr18/Dh06xOOPP87+/ftp1qwZP/30E9WqVQNg165d+Pikx3aRkZHMnTuXe++9lyZNmhAeHs6YMWN48MEHvfUSioWYGPj88wIGThWvgPKXw4m/Yed0qHuH29onIrkIjYagCEjck00Bm3k+NNqjzRIRESlNvJ4cYvTo0YwePTrL5xYtWpRpX5s2bfgt32POSifnek6//QZnz0JgYD4qca7p9Od4s6aTAicRz/GxQ9QgWP989mVaTDTlREREpFB4daieeEbdulC9Opw7B1lkeXdd1ECTsevIb3DiX7e1T0RycXILbH7T3Pctm/n5endDZKxn2yQiIlLKKHAqBWy29F6nAg3XK1MdwrqZ+9s/LnC7RMQFKYkQ1weST0CVNtD7EFy7ENpOhdq3mjJH1AsvIiJS2BQ4lRJuCZwgPUnE9k8g1VHAykQkR5YFf9wOx9dCYFVo9xX4lTEpx6P6Q7PnwccfjvwBh//wdmtFRERKNAVOpYQzcFq+HJKSClBR+A3gXwnO7IP9893SNhHJxqbXYcfnZojs1V9CUHjG5wOrwiX90suKiIhIoVHgVEo0aAChoXDmDKxYUYCKLlzTadtHbmmbiGTh0FJYPc7cb/4yVIvJutxld5vbXdPh7EHPtE1ERKQUUuBUSths0L69ue+24Xp7voGkYwWsTEQyObMffr0JrBTTo3TZ2OzLVm4Fla+E1CTY8p7HmlgqpDrgwCLYMc3caniyiEippsCpFHHbPKeKV0CFxpB6zqzpJCLuk5oMv94MZ+KhfCNo/b755SMn9c73Om1+C1JTCr+NpcHuWTA7ChZ0hGUDzO3sKLNfRERKJQVOpYgzcFq6FJKTC1CRzQa1hpr726YUsFUeoF+NpTj58wE4FAd+IRA9C/yCcz/mkpvMfKcze01PsBTM7lkQ1zfzgsOJe81+BU8iIqWSAqdS5PLLoVIlOH0aVq8uYGVpazr9Dic2uKV9hUK/GktxsmMabJxo7rf5BELquXacPQDq3G7ub5pcKE0rNVIdsGoMYGXx5Pl9q8bqBxgRkVJIgVMp4uMD0dHmfoGH65WpBjW6m/tFdU0n/WpcuhW3nsbj6+D34eZ+w4chomfejq97p/kx4+ASOLbW/e0rLQ7FZf6bkYEFibtNORERKVUUOJUyHTqY2wIHTnDBmk6fFr0vpfrVuHQrbj2NSSdgSSw4EqF6Z2jydN7rCAqHyFhzX6nJ8+9MvHvLiYhIiaHAqZRxznP69VdwFDRmqHEDBFQ+v6bTvAK3za30q3HpVdx6Gq1UWD4YTm2BoEug7VTwseevLmeSiB2fKeNlfp3Z51q5MmGF2w4RESlyFDiVMk2aQPnykJAAa9YUsDK7P9R0ruk0pYCVuZl+NS6dimNP4z/Pw97Z4BMA0TMhsEr+6wptBxWagOMMbP3QfW0sDRL3wq/94M/xuRS0QVAkhEZ7pFkiIlJ0KHAqZez29HlOixa5ocKiuKbTqe2ur2fjOFu4bRHPKm49jfE/w9rHzP1Wb0DllgWrz2a7IDX5m0UrQCyqHEmw/mX4/jLY9SXYfCCsG2A7v13Mgub/zX+voIiIFFsKnEoht63nBFCxedFZ0ynpBPz5IHxfHw4udO2Y32+FpQMgYWPhtk0848Q/rpXbNgXOHirUpmSS6sB2cDHhKUuwHVwMJ7fC0v6ABXVGQJ3b3HOeqAHgXxFObYP4H91TZ0l1YCH82AzWPAApp6FKW+i6CjrOgegZZt5YBucDqaISeIuIiEcpcCqFnIFTXJwb5jkVhTWdUpNh0xvw3aWw4SVITYJq10LzV8j6V+Pz+ypfZR7unAY/NITlQ8yXWSl+Tm2DP+4ww/Bcsf1j+LoGLL4Rds3IuefRHdn5zier8F3cmZbnXsV3cWf4vgEkHYVKraClG1OI+walB2EblZo8S4n7zA8mC66BhA0QEApXfQSd46BiM1MmMhZu3AHXLjTzzq5dCO2+NM9tmgxb3vVW60VExEt8vd0A8bzmzaFcOTh+HNatg2bNClhh1EDzi61zTafyDdzQShdYFuz7Af68HxL+NftC6puAqUZ3E9QF1zJzXi4cvhUUAS0mmi9Gx9bA2ifMHJPtn8COz6H2MLj8UShbM+P5Uh3ml+Yz8WZieGi0hut424n1Zo7QzmlgnQ9ofPxN8JwlG/iVh+A6cGwV7P3ObH4VoObNUGuw6XWwnQ+2d8/K5vMzKT2DXW6cySounndlnV+Fus5tZh0md6o7Ejb8F/b/bHpTQy5zb/1FXYbevbIQ1tH8W01NNsHkuicg5ZQZlnfpSGj6tOmlu5iPHap1yLivydNmeOWKUVDuMqgW45GXJCIi3qfAqRTy9YWrr4affjLD9QocOJWpBjWuN8HHtinQ/EU3tJLsv/yACXhW3wcHfjGPA6pA4wlw6Qjw8UuvIzIWwntmH/BUbAYx38KRFSaAiv8Rtr5veiTqDIdGj5jhOu74Ai2ucSVAPboa/nkWdn9NWkAS1tW8X+cOng9UIGOwcj4YuuoD856d2GBS6e/41LyvW941W3BtiBoEgVVh5WgyBTzO7HzRM3J/73NMVnHeP8+az5o7g/Dg2hB+gwkKN70BLV9zX91F3fl/q76Je2gJsPhV82+1zu2wa3r6cM7KV5l5ZZWuyFv9jR6B43+bun7tA11WmB9oRESkxFPgVErFxKQHTmPGuKHC2kNN4LTjU2j6LPgU8KOV3Zefy5+Aw8vODwu0TO9C/XvNgqH+5bOuK6tfjS9WuZWZ13BoGax9HA4sgM1vmcxk1a49P1ekAF+gC0NJ7AHLLUA9+KsJNOJ/Sn8+MhYa/R9UapG+L3pGzj2NYHpGmz0HTZ8xQ/C2fwK7Z5phf39PyKGRFmAzwwLDe5peC8cZSE6A5JOQkpB+//DvuSSrID1ZRW6f0byqd7cJnLZNMf8m/cq5t/6iKLvevcQ9sO5xcz+gCjR70fzNsuVjtLrNBld9aNLHH11lhntet6x0XF8RkVJOgVMp5ZzntGQJpKaCT0Fnu9W4/vyaTvFmTaca3fJfV05ffv4Ykf645n+g6fMQHJX/c10stC1cOx8OLDbDcQ7FQfycbApf9AXa1aDFHQFPUewBK+jryvZ93wtxfSCkISSsN/tsPlCzvwmYKzTKXFduPY0XsvlA9WvM1uoN2P0N/DsRjq3MobHns/N9VR5Sz6YPE8yvwkiLX/1aM0QvYaMJCuuNcv85nIpCEO9K755vMFy/HgJDC3Yu3yBo/w381ApO/A3LboH2X+cvEBMRkWJDf+VLqZYtISgIjhyB9evdUKHdH2oONPcLkiTClS8/Pv7Q+Ve4epp7g6YLVYuBTovPJ5jISR7TW59PEsCCjrBsgLmdHZW3RVmL4gKvBX1drqy/lLAebL4mA90Nm6DtZ1kHTU7Onsao/ubWlS/yvmWh1kBoMM61djtOXxA02cAvBMqEQ0gDqNwaKro4DKwwFlO1+UDd0eb+ptfNnMDC4I7PtFNBEnEcWJB7717KKdczL+YmKMIETz4Bprf9r0fdU6+IiBRZ6nEqpfz8oG1bmD/fDNe7/HI3VFp7KGx6zcw72fOd+ZKS11+fDy7J/ctPapKZ5F3YbDYoU8O1skt6Q8WmUK4elKsLIedvg+ukT/zPsUfFHXNm8tkDVlAFfV0pp00vT27vO5jsZjVvKkhrXeNqIHPVx1C9kxmm5Vs2c49DqsMEEYl7yfo9s5kv4IW1mGrtwfDXwyZ5yoEFpq3u5I7P9IV1udqLmpxg5jkeXQ3H/jTb8b9dO487e/eqtIbW78PyQbD+eahwuUkHX1iKQs+eiEgppsCpFIuJSQ+cRrljFE/FZhBUExJ3wpIb0/fnNITMcdbMEzi8HA7/BvsXuHauwhjalBVXv0AnH4eDi82Wgc1k5wu+FI4sJ8celT9Ggk+gWRMrJREciedvT5vblEQ4ucX1BV7dPWcmK64Ecn/cAeeOwbkDcGY/nN1/wW28CbBdZaW4qeG5CI02n9vcAp6ogTl/cfWxm89+XF9zTFbJKlpMLLwvv34h53/QeN1kk3Nn4OTKe7/ybqh2jclk6MxUmJXcArBGj5jhccf+NMHSqQIsG+Du3r1at8DxdWYphN9vMz+YVG7l3nNA0RyeKyJSyihwKsU6dDC3ixebUTw5fa9xyZ6vTdB0MeeXn3ZfQeWWcGg5HPnNBEvH/sxf71FhDG3KiitfoMvUMK/t1FY4ufn8tsncJifA6R1my825g7D4eve0e8t7pgekYvPcE3XklL0wNwcW5R7InTsMfwzPuZ4cU4hfwFPvuzsDnshY15JVFJa6o0zgtPc7OLXDfcNbD8Xl/t6f2QczKpqeON8Qk8DF74LNvzz4ljPLAOT0o8I/z2R+KijSZMSr2Pz8QtxNYF40nPFC717T58wQwH0/wJKeJtNepsVzC8CdPXvuVJC/HSIixZACp1KsVSsIDISDB2HjRqhfvwCVpf36nJXz/9n/ejOQmvnpwGpQpQ1UuQoqXwnLBpkvXN4Y2nQxV75At3wNQtuY7UKWBWcPmgBq+6ew1YUFM8vWNK/PHmR+Yb/49swB2D4l93p2TjWbbzkIvRqqxpitcsuM6dqzy16Y1a/YjiTz5fDYavOr/9FV5tYVFZqaL7mB1aFM9fO3YemPfcrAd7W8N6QtK+4MeM4nq0iJX8ia336k2VXd8PXUl8zy9aF6Z5O0ZfOb0Pwl99Sbl15fK9X0yiYfz//5qnWEsG5QqTlUaAaBVTKXaeml3j0fO1w9FX5uY9YWW9ILOi0B3zIFr7soDs+FvP3tEBEpIRQ4lWIBAdCmDSxcCIsWFTBwyvXXZzBBk4/54lOlTfpWNipjd1fL17w3tCkr+f0CbbOZNa7KVDNDzFwJnK6akvMQu1QHHJifQ4CBWcw1tB0c+tV8UY3/KT19tz3IZA6sej6t4trHM9fj/BW76fOmR+DoahMsHV/nWq9QVlpMzH3ooDeHtGUnL9n5cuNjx6oaw17f0zStGuPZ11JvtAmctr4PjZ80QXhBnT3gWrkOc0zgnHzCbEkn0u8nn4BDS2HPN7nXU2eESfSRE2/27vmFQMx3JtPe0ZVm2F7bzwvele9Kz54nh+dC0e0BExEpZAqcSrmYGBM4LV4Md95ZgIpc/fX5qg/MnIuceHtoU3ZtKsgXaFfnzOTWo+JKD5hzgddUB5xYZ1KrH1wMh5bAuSOwf77ZsnW+zr8eyvyUf0WTKa7SFea2YjP4pbN7hkcVxfcdXFsHrKircb35geL0Dtg5DercVrD6tn4Ef96fS6Hz7331687/O8km0Uqllq4FTq4O0/Rm715wbfMZ/uU6c50rNIZGD+evLmciiHVPuVbeU/M+i2oPmIiIByhwKuWc6zkVeJ6Tq19qyka5Vs6bX36yU5Av0N6YM+NjN4FNxWZQf4wZLnVivQmidn2VRSKLLFRqBWGd04Oli3sHwb3Do9zZwyPpfOxQ9y5Y84BJElH71vz9Y7dS4a9HYP0L5nGVq82C1ObJCwrm4b13148KF/Jm7161jqbXfMVd5lqVbwg1bnDtM21ZZgjsjqmwa/r5Icsu8tT8v6LYAyYi4iEKnEq51q3B3x/i42HLFqhbN58VlbQvP4WhEObM5CnAsPmYdMkVLgf/Sq4FTvXv9fzwqJLQw1MU1bkN1j0Bx/8yw+Oqtsvb8SmJsHww7J5pHjd6FJpMML1FBXnvvZ15sDDUHWmGtm5+C37tB/4VMg5tvHgu0IkNpodqxzQ4tSW9nF8FiOxtEnucO0L269v5mKDWE1zt2fJUD5iIiAcpcCrlypQxwVNcnOl1ynfgVBK//BQGN8+ZyXeA4eqv03kcHqWeoiIsoJJJn771fdg0OW+B05l4WHyjmbvj42/WLqo1yDznjve+qA7TLIgWk+BgHJz4O/N8MOdcoKhbzHDaY2vSn7OXgfAbzQ8WYV3NOnBpc4qy+ttqAamwsDNc/oRJ3V6o/+5c7Kn0VA+YiIgHKXASYmLSA6fhuWSNzlFJ/PJTGIpCj0oh9RB6/XVJzuqNNoHT7lnmvXclZfaxv2BxDzP8KqAyRH8NVS/6XLjjvS9xwbcPJB3N5rnz/+Z2fGpubb4Q1gVq9oeInuAXnLF4Tn9bm71gEn9sm2J6FA8uMkkp3B24OM7Bhlfg7yxSw18sKNKzGTBFRDwkX4HT7t27sdlsREREAPDHH38wdepUGjZsyO233+7WBkrhi4mBZ55x03pOJe7LTwmlHsLSqWJT8+/xUBxseQea5JJ4YO/3sLS/WaQ4pD7EfA/l6hRe+0pS8H0ozrU5SpfdC5c/YoLSnOT0tzVqgFloeMVIOLAQ5jSFNp9Aja7ueS3x82DlaLM+HUBII0hYf/7JLH54ufwx/e0QkRLJJz8HDRgwgIULFwKwf/9+OnfuzB9//MEjjzzCU0+5mAFIiow2bcDXF3bvhh073FCh88tPVH9zq/9Aiybnr9gX9zoERSidcElWb7S53fKO6UXIimXBv5PMYq4pp8yX8uuWFW7QVNK4OsencqvcgyannP621hoEXVeZ1O/nDsGibvDng/lbYNwpcY9Zf2/hdSZoCqxuerOuX5f13w7b+TXitrwLKWfyf14RkSIqX4HT33//zZVXXgnAl19+yeWXX86yZcv4/PPPmTJlijvbJx5QtqxZDBdMr5OUIpGxcOMOUmLmsTJgHCkx8+DG7QqaSrLI3lCmhlmcedeMzM+npsDKUbB6rEk4UGcEdPzJpKIX17l7HqErQi6DLr+ZDIoAG16Cee3h1I681ZOabIblfV/fZOC0+cBlY+CGf03vls2W9d+OG9abIPDoSvj9VhOAi4iUIPkKnJKTkwkICABg/vz53HjjjQDUr1+f+Hhl0imOnGnJv/gCpk0zC+I6HF5tknhKWvbC9lglIXuh5MzHDy49v2jbxtfgwCKTze3AIjh3FBZdb7LBYYPmr8CV75hjJG+c8wizTaZgK5y5QPZAaPUGtJsBfuXhyG/wY3PY/XV6mVRHxvc99YI/9geXmPJ/3g8pp80i5V1XmaG7/uUznuvivx3lLoV2M82crZ1fwPrn3fvaRES8LF9znBo1asTbb7/N9ddfz7x583j66acB2LdvH5UruzjkQIoU3/OfhLlzzQYQEQGTJkGsOh9ESpZLb4e/J8DRP2BBx/T9Nl+wUsAeZIZkRfbyWhOLPW/PI7ykD1RqAUv/A0d+h7hYqDvKZFP88/7MSSYuf9IkltjxmdkXUAWavWgWLLfl4TfWajHQ8nVYcef5dawamYQXIiIlQL56nF588UXeeecdOnToQP/+/WnatCkAs2fPThvCJ8XHrFnw7LOZ9+/dC337mudFpAQ5vBSsLLqUrRRz23iCgiZ38PY8wuAo6BwHDR4wjze/YZJ9XLyAbeIe+GP4+aDJBpfeATdshDq35i1ocqp7hwnSAJYNhGNrC/IqRESKjHz1OHXo0IHDhw+TkJBAxYrp495vv/12goKC3NY4KXwOB4wZk/VQdGeGvbFjoWdPsGsEl0jxl+owaa2zZYNNr5nFjzVss+C8nWnUxw+av2hSyC/uCeSwUK7NDzothtA2BT9vi/9BwgY48AssuRG6rIDA0ILXKyLiRfnqcTpz5gznzp1LC5p27tzJxIkT2bhxI1WrVnVrA6VwxcXBnj3ZP29ZJtteXJzn2iQihehQXOYehwwss2bTIf2jd5uikGnUN5gcgyYAKxlSs8m0mFc+ftDuKwiuA6d3QlwfcCS5p24RES/JV+DUs2dPPvnkEwCOHz9O69at+e9//0uvXr1466233NpAKVyu5vJQzg+REsLVNNmulpPiwRvve0AliJkNvuVMIL5ytDLtiUixlq/AafXq1URHm0xAM2bMoFq1auzcuZNPPvmE1157za0NlMIV5mImXFfLiUgR54002eJ93nrfyzeEq78AbLD1Pdj0unvrFxHxoHwFTomJiZQrVw6An3/+mdjYWHx8fLjqqqvYuXOnWxsohSs62mTPs2WTMddmg8hIU05ESgBvpckW7/Lm+x7eHZq/ZO6vvhf2z3f/OUREPCBfgdOll17KN998w+7du5k7dy7XXXcdAAcPHiQkJMStDZTCZbeblOOQffA0caISQ4iUGM402UDmL9EeSJMt3uHt973+fVBrsMnmGHcTJGwunPOIiBSifAVOjz/+OOPHjycqKoorr7ySNm1MBp6ff/6Z5s2bu7WBUvhiY2HGDAi/KGOury989ZXWcRIpcbydJlu8w5vvu81mFlOufBUkHzeZ9pJOFN75REQKQb7Skfft25d27doRHx+ftoYTwLXXXkvv3r3d1jjxnNhYk3I8Lg62bIG774azZ6FsWW+3TEQKhbfTZIt3ePN9twdC+69hbitI+NcszhvzvT5zIlJs5CtwAqhevTrVq1dnz/lc1hEREVr8tpiz26FDB7P9848Zovfqq9C1q5cbJiKFw5kmW0oXb77vZapD+29hXjuI/wnWPAjNXlQALyLFQr6G6qWmpvLUU09Rvnx5atasSc2aNalQoQJPP/00qam5rBMhxcKYMeDjA/PmwVot+i4iIu5S6Qq4aoq5/+9/YVYoLOgIywaY29lRsHuWN1soIpKlfAVOjzzyCK+//jovvPACf/75J3/++SfPPfcckydP5rHHHnN3G8ULoqKgTx9z/9VXvdoUEREpaWreDJF9zf2kYxmfS9wLcX29FzylOuDAItgxzdymOrzTDhEpcvI1VO/jjz/m/fff58Ybb0zb16RJE8LDw7nrrrt49tln3dZA8Z777jPJIaZOheef11pOIiLiJqkOOPxbNk9agA1WjTXzsTw5bG/3LFg1BhL3pO8LijAZCZU0RaTUy1eP09GjR6lfv36m/fXr1+fo0aMFbpQUDa1bQ9u2kJwMr2vNQhERcZdDcXBmTw4FLEjcbcq5qqA9RbtnmZ6uxIva5e0eMBEpMvIVODVt2pTXs/gm/frrr9OkSZMCN0qKjvvuM7dvvw2nT3u3LSIiUkKciXet3G+3wm+3wcbX4MDizMP6nHbPMnOj8jtXKtVhepqwsnjy/L5VYzVsT6SUy9dQvZdeeonrr7+e+fPnp63htHz5cnbv3s2cOXPc2kDxrp49oXZt2LYNPv4Y7rrL2y0SEZFir4yLY79Pb4dt2zPuC7oEKjaFCk3M7ZkDsOoeMgU9zp6ii9eocpwzAVjSUTh31NweWpa5pymDC3rAlIlSpNTKV+AUExPDpk2beOONN/j3338BiI2N5fbbb+eZZ54hOjrarY0U77HbYexYuOce+N//4I47zD4REZF8C402c4cS95J1L48NAqtDi4lw4m849hcc/wtO74TEXWbb+10uJzlf79IBUK6uWXj33FFwJOa/3a72lIlIiZTvdZxq1KiRKQnEX3/9xQcffMC7775b4IZJ0TFsGDz+uFkY9/vvTS+UiIhIvvnYTcKFuL6AjYzBk83ctHr9fE/RzelPJR2H42vTA6mDcXByU87nSj1ngq8L2XzArwL4V4KASmBZcHRF7u22+eVeRkRKrHwHTlJ6BAfDnXfCCy/Af/+rwElERNwgMtYMo8syi93ErLPY+VeAqu3NBiYRxLIBuZ+r4UMQ2ccESf6VwC/EBE9OqQ4zJyrbHrDzlg+CY6ug4QPgXzH384pIiZKv5BBS+oweDb6+EBcHK1z4UU5ERCRXkbFw4w64diG0nWpub9zueupvV+dKhXWByi0huLYJvmwXff1x9oABaT1eac4/DqkPqWdh/QvwbW1Y/yKkFGDYn4gUOwqcxCXh4dC/v7mvBXFFRMRtfOwm4UJUf3Obl3WbnHOlMgU7TjYIijTlcuPsAQsKz7g/KAKiZ8L16yHmOyh/uZkvteYh+O5S2Pw2pCa73mYRKbbyNFQvNjbnX4COHz9ekLZIETduHHz6qVkU98UX4ZJLvN0iEREp1VyZK9ViouvBWGSsWXT3UJxJBFEmzARdzuPDb4CwbrBzGqx9DE7vgBUjYcMr0ORpqNkvvTcr1ZF9PSJSLOUpcCpfvnyuzw8ePLhADZKiq1kzuOYa+OUXmDTJzHcSERHxqvzMlcqJswcsp+dr3QKX3Axb34O/n4JTW81cq/UvQrPnzRC+1WOzaM+kvLdHRIqMPAVOH330UWG1Q4qJ++4zgdN778ETT0BIiLdbJCIipV5uPUWFwe4P9UZBrSGwcRJseMlk+lvUPevy2a0rJSLFhuY4SZ507QoNGsDJk/D++95ujYiIyHkFmStVEH7BcPkjcOM2qD8uh4LnhxGuGmuG8YlIsaPASfLExwfuvdfcnzQJUlK82x4REZEiIaAyhPfIpZAFibtNz5iIFDsKnCTPBg2C0FDYtQtmzvR2a0RERIqIM/HuLSciRYoCJ8mzwEAYNcrc/+9/zYLrIiIipZ6r60q5Wk5EihQFTpIvI0dCQIBZDHfpUm+3RkREpAjIdV0pwOYHZaM81SIRcSMFTpIvVauCM/O80pKLiIiQvq4UkG3wZCXDvLZwdJXHmiUi7qHASfLNmSTi229h82bvtkVERKRIcK4rFRSecX9QJLR6G8pfbuY4zWsPe771ThtFJF8UOEm+NWgA3bubOU6TJuVeXkREpFSIjIUbd8C1C6HtVHN743aoewdctxSqXweORFjSG/79nyYLixQTCpykQO67z9x+9BEcPerdtoiIiBQZ2a0r5RcCHX6AS+8ALFg9DlaOhlSt7yFS1ClwkgLp2BGaNYPERHj7bW+3RkREpBjw8YVWb0HzVwAbbH4TFt8IyQnebpmI5ECBkxSIzQbjzi+UPnkynDvn3faIiIgUCzYbNLgPomeCvQzE/wjzouH0bm+3TESyUSQCpzfeeIOoqCgCAwNp3bo1f/zxh0vHffHFF9hsNnr16lW4DZQc9esHNWrA/v3wxRfebo2IiEgxEtkbOi2GwGpwfC383FoZ90SKKK8HTtOnT2fcuHE88cQTrF69mqZNm9KlSxcOHjyY43E7duxg/PjxREdHe6ilkh1/f7j7bnP/1Vc1x1VERCRPKreCLr9D+UYXZNybbZ5LdWA7uJjwlCXYDi6GVId32ypSink9cHr11VcZMWIEw4YNo2HDhrz99tsEBQXx4YcfZnuMw+Fg4MCBTJgwgdq1a3uwtZKdO+6AsmVh7VpYsMDbrRERESlmytaEzhdm3OsFy2+Fb6PwXdyZludexXdxZ5gdBbtnebu1IqWSrzdPnpSUxKpVq3j44YfT9vn4+NCpUyeWL1+e7XFPPfUUVatW5bbbbiMuLi7Hc5w7d45zF0y8SUgwEy+Tk5NJTk7OV7udx+X3+JIoOBiGDvXhjTfsvPxyKpBKfDyEhUG7dhZ2e/7q1bX2DF1nz9B19hxda8/QdXYzWxBc/TU+f47Bvu192P4RFhmX0rUS90JcXxxtvsCK6O2tlpZY+kx7RlG6znlpg1cDp8OHD+NwOKhWrVqG/dWqVePff//N8phff/2VDz74gDVr1rh0jueff54JEyZk2v/zzz8TFBSU5zZfaN68eQU6vqS5/PIgoBM//+zDzz+nd2ZWrnyG4cPX0aZNfL7r1rX2DF1nz9B19hxda8/QdXaz1K50Yyp+JGYImgBsWFhA0m+jmFfGF2z5/GVScqTPtGcUheucmJjoclmvBk55dfLkSQYNGsR7771HlSpVXDrm4YcfZpwz7RumxykyMpLrrruOkJCQfLUjOTmZefPm0blzZ/z8/PJVR0n09dcX/3k3jh4N5KWXWvHFFw56987bBChda8/QdfYMXWfP0bX2DF3nwmE7uBjfxdl/mbMBQdZhrm8VglU1xnMNKwX0mfaMonSdnaPRXOHVwKlKlSrY7XYOHDiQYf+BAweoXr16pvJbt25lx44d9OjRI21famoqAL6+vmzcuJE6depkOCYgIICAgIBMdfn5+RX4jXJHHSWFw5G+GO7FLMuGzQbjx/vSpw/5Grana+0Zus6eoevsObrWnqHr7GbJh1wq5pt8CHTdC4U+055RFK5zXs7v1eQQ/v7+tGjRggUXZBNITU1lwYIFtGnTJlP5+vXrs27dOtasWZO23XjjjXTs2JE1a9YQGRnpyebLBeLiYM+e7J+3LNi925QTERGRHJQJc63cto/h8O9KZyviIV4fqjdu3DiGDBlCy5YtufLKK5k4cSKnT59m2LBhAAwePJjw8HCef/55AgMDufzyyzMcX6FCBYBM+8Wz4l2cvuRqORERkVIrNBqCIiBxL5BDULR/rtkqNoNL74SogeAX7KlWipQ6Xg+c+vXrx6FDh3j88cfZv38/zZo146effkpLGLFr1y58fLyeNV1yEebij2OulhMRESm1fOzQYhLE9cXMaLoweDo/n7jpc5DwL+yaDsfWwIo74c/7odYtJoiq2CRjnakOOBRn1okqE2aCMx8llhDJC68HTgCjR49m9OjRWT63aNGiHI+dMmWK+xskeRYdDRERsHdv1iMGbDbzvNYrFhERcUFkLETPgFVjIPGCsfBBEdBionke4IpXYfvHsPltOLkJNr9ltiptoe6dcMlNsG9ONvVMSq9HRHKlrhxxC7sdJk0y921ZJNezLHj++fwlhhARESmVImPhxh2kxMxjZcA4UmLmwY3bMwY7AZWg/r1ww79wzQITKNl84fAyWD4YZoZCXJ+MQROYYYBxffO3mG6qAw4sgh3TzG2qoyCvUqTYUOAkbhMbCzNmQHh4xv3OkZZTp0JKiufbJSIiUmz52LGqxrDXt71JPZ7d8DqbDapfA+2+hF67ockzUCYSUk5lU/H54SGrxuYt8Nk9C2ZHwYKOsGyAuZ0dlb8ATKSYKRJD9aTkiI2Fnj1N9rz4eDOnKSAArrkG5syBsWNh8uSse6VERETEDcpUh8sfgcpXwcJOORS0IHE3zG1t5kSVCYegcDOMr8z524Aq6f9p7551ft7VRWPynb1X0TPyNvRP866kmFHgJG5nt0OHDhn3ffYZ9O0Lb7wBdevCmDFeaZqIiEjpce6ga+WOrTJbVnz8oUwNE0gdW03WWf4swGZ6r8J7uhb87J6leVdS7ChwEo/o0wdeegkeeADuvRdq1YIbb/R2q0REREowV9eDavAg+IeYICZxr7k9sxfOHoTUJDi9w2w5Ot97teh6qNIayl4CQZdA2ZoQFAm+ZdKLurvnSsRDFDiJx4wfD1u2wLvvQv/+ZjjfFVd4u1UiIiIlVK7rQdnM802fzbqXyJEEZ+PN8Tunw6bXcj+nc22piwVWNYFUUCTsn5dNe/LRcyXiQUoOIR5js8Hrr0PnzpCYCDfcALt3e7tVIiIiJZRzPSggbf2nNOcft5iYfYBi9zc9RqFtIbK3a+escxtcegeEdYPyDcG3rNl/9iAcXQl7vs4hYQWk9VwdinPtfCIepB4n8Sg/P/jqK7j6avjnHxM8/forlCvn7ZaJiIiUQK6uB5UbV3uvWr2TMRCzLEg+Dqd3wuldZpje9o9zP9/+BUoWIUWOAifxuPLl4YcfoHVrWLsW/vMf+PZb8NWnUURExP0iY83Qt4JksHP2XsX1xfRWXRg85dB7ZbOBf0WzVWwGfiGuBU7/PANb34dL+sIlN0Po1WDLZqCUsvOJh2ionnhFzZowezaUKZOeptzK6gcsERERKTgfO1TrAFH9zW1+Agtn71XQRQs2BkW4ntDB2XOVaejgBexlwa8CnN0Pm16H+e3hm0gz9+nQMrBS08tqXSnxIP3GL15z5ZXw+ecm457SlIuIiBQDBe29cqXnqu0nUOMGOLAAdn0Ju7+GM/tg4ySzBUVA5E0m4cRf/4ey84mnqMdJvKp3b5OmHEya8m+/9W57REREJBcF7b1ypefK7g81usFVH0HsAYj5DqIGgW85M1dr4//gr4fJPjsfpocq1ZHnlyeSHfU4idfdd59JU/7OOzBgACxZAi1aeLtVIiIiUmjy0nNlD4DwG8zmOAvxP8PG10yPVLbOZ+c7uASqd3StTakObAcXE56yBNvBshDWUXOlJAMFTuJ1NhtMngzbt8PPP0OPHvD771CjBixebGPJknDKlrXRsSPY9fdLRESkZHD2XOWFPRAiboSU07kETuct6QlVo6FSS6jcCiq1gjLVMpfbPQtWjcE3cQ8tARa/ej7z4CQN95M0CpykSPDzgy+/hHbt4O+/IToakpNh3z5foCWvvgoRETBpEsTq75eIiEjpVibMtXIpJ2HfHLM5BUWmB1KVW5ker+VD0FwpyY0CJykyypeH77+Hpk1h587Mz+/dC337wowZCp5ERERKNVfWlSoTDu2mw9HVcHQFHFkBCf+aIXyJu81ivDmyTD2rxpphha4O21N69BJLgZMUKREREBCQ9XOWZYb1jR0LPXtq2J6IiEip5Up2vpaTILSt2ZyST2YMpA4uMWnPs3V+rtT6F6DWINNbZcshlfr5IX+ZFxvOx5A/BWBFjgInKVLi4uDgweyftyzYvduU69DBY80SERGRosaZnS/LQGVi1oGKXzmoFmM2gB3TzPpPuVn7qNn8K5mFfC/cQuqDj58JmuL64pYhf+4MwMRtFDhJkRIf71q5mTPhsssgzIUhzg6HCbTi40356Gj1VomIiJQIBV1XytW5UmVrQ+IuSDoKB34xm5NPAJRvBAkbyT49eh6G/LkzABO3UuAkRYorgRDA66+brW5d0/MUE2O2iIiM5WbNMovq7rngBxslmRARESlB8pOdz8mVuVJBEdBjE1gpcOIfOLYm45ZyEo6tzuVE54f8/XozVGxieq4CKptb5/2ASmAPNj1N7gjAxO0UOEmREh1tApu9e82wvKwEB0OdOrB2LWzebLb33jPP1amTHkSdOQMjR2auR0kmREREBHBtrlSLieeDFDtUusJsTlYqnN4BGyfDxom5n2/PLLNl6+I2XOx8AHYoLv/BYn5pzpUCJyla7HbTG9S3r5l7eWHQ45yL+fHHJuA5dgx+/RUWL4ZFi+DPP2HrVrN9+GH251CSCREREUmTn7lSTjYfCK4NET1dC5xqDjDzrM4dMcP+ko6m3085Tc5B0wXOuDi3wV005wpQ4CRFUGys6Q3KaojdxInpvUQVK5rFcnv0MI9PnIClS00Q9d138O+/2Z9DSSZEREQkzfm5UinxC1nz2480u6obvmEdXe9RcXXIX5tPsq/TcQ72fg+/9s39fH89Aqe2wiX9IKRuzmUL2lPk7jlXqQ5sBxcTnrIE28GykJfr7GUKnKRIio01vUELF6bw449r6NatGR07+ubYO1S+PHTvbrbmzWGAC0lyXE1GISIiIiWcjx2ragx7fU/TtGpM3r7M52nIXzbsARDRK5cA7LzT22HtY2ar2Bxq9oNLbobgWhnLFbSnKNXh3jlX59vjm7iHlgCLXy1WPVc+3m6ASHbsdoiJsWjffi8xMVaehtS5mmTC1XIiIiIiOXIO+QsKz7g/KML1XhlnAAakBVxpbGZr8zG0/hDCuoDNDsf+hDUPwezaMLc1bHgVTu9O7ym6MGiC9J6i3dnMtUpOMGtd7fwS/rg98/EZnJ9z9ftw2PohxM+DE/9C8qnMRfPbniJEPU5SIrmSZCI01JQTERERcYuCpkd31uHKnKs6w+DsYZNsYud0OLgIjvxhtj/vAx9/cuwpWnEXpJyBU9vg1BY4uRlOboFzh/L+urdPMduF/CpA2UgoE2GCyV1f5tyeYpAtUIGTlEg5JZlwOn4c5s2Drl093jwREREpqQqSHt3J1QAssApcervZzhyA3TNMgHJwCaQm5XACC84egOW3ZP10YFUIvhR8y8H+ubm3t0Z3sBym9ylxj+m1Sj5uvmwdX+fCC/ZitsA8UOAkJVZOSSaqVoXVq+HGG+GLL5SWXERERIqYvAZgZapBvVFm2/QGrByd+zHlLoPQtlCuLpS71ARL5eqAX4h5PtUBs6NyT3rRfnbGoC45wQRQp3ebgGjfj7mkYT/P09kC80iBk5RoziQTcXEmEURYmBme53DALbfAV1/BzTfDlCnmsYiIiEixV76Ra+WufDvn4Cy/SS/8QqB8Q7OBCcpcCZzKFO3J5wqcpMSz2zOnHLfbYdo0KFvWBE2DB0NiItx+uzdaKCIiIuJGrqZHD3VhsndB1rkqjPZ4kQInKbXsdvjgAxM8vfEG3HEHnDoF48Z5u2UiIiIiBeCO9OgXKmjSC3e3x0uUjlxKNR8fmDwZHnzQPL7vPnjqqewz8YmIiIgUC+5Ij34h55yrqP7mNq9Bjrvb4wXqcZJSz2aD55+HcuXg0UfhiSdMz9OLL5rnRERERIold6RHL4T2pMQvZM1vP9Lsqm74hnUs8j1NTgqcRDAB0iOPmGF7994LL78Mp0+b3igf9cuKiIhIceWO9Oju5GPHqhrDXt/TNK0aU2yCJtBQPZEMxo6Fd981gdSbb8Ktt0JKirdbJSIiIiLeph4nkYuMGAFBQTBkCHz8sel5+vxzk0zi4rTm9uLzI4mIiIiIFIACJ5EsDBxogqf//McsorttGxw4AHv3ppeJiIBJk7R4roiIiEhpoKF6Itno3RtmzwZ/f1i9OmPQBOZx374wy4X13ERERESkeFPgJJKDTp2gfPmsn3OmLB87FhwOjzVJRERERLxAgZNIDuLi4NCh7J+3LNi925QTERERkZJLgZNIDuLj3VtORERERIonBU4iOQgLc61cUFDhtkNEREREvEuBk0gOoqNN9jybLedyQ4aYDHvJyZ5pl4iIiIh4lgInkRzY7SYggszBk/NxzZpw4oRJEtG4MfzwQ3riCBEREREpGRQ4ieQiNtas5RQennF/RATMnAlbt8K770JoKGzcCDfcAF27wj//eKe9IiIiIuJ+CpxEXBAbCzt2wMKFMHWqud2+3ey322HECNi8GR54wKz79PPP0LQpjB4Nhw9nrMvhgEWLYNo0c6tU5iIiIiJFnwInERfZ7dChA/Tvb27t9ozPly8PL74I69ebxXMdDnjjDahbFyZOhKQks1huVBR07AgDBpjbqCgtoisiIiJS1ClwEnGzOnVMIPTLL6bX6fhxuPdeEyD16QN79mQsv3cv9O2r4ElERESkKFPgJFJIOnaEVavgvffM/Kfs1npyJpIYO1bD9kRERESKKgVOIoXIbofhw+Gjj3IuZ1mwezfExXmmXRdyOGDxYhtLloSzeLFNwZuIiIhIFhQ4iXhAQoJr5bLrlSoszjlXnTv78uqrLenc2VdzrkRERESyoMBJxAPCwlwrt2mT54brzZpl5lZpzpWIiIhI7hQ4iXhAdLRZ9+niRXQv9uSTUK8evP46nD5deO1xOGDMmKwX6tWcKxEREZHMFDiJeIDdDpMmmfsXB082m9n69oVKlWDbNrj7boiMhEceyX74XkHWg1q4MHNP04W8OedKREREpChS4CTiIbGxMGMGhIdn3B8RYfZ/9RXs2mXWfqpTB44dg+eeM3OQbrvNrA/llNf1oJKT4bff4IUXoEsXuOEG19rs6TlXIiIiIkWVr7cbIFKaxMZCz56mJyc+3sx9io5OX0y3bFm46y644w6YPRteeQWWLYMPPzRb9+5w5ZUwYULmYXbOuUkzZkCPHrB6telZWrQIfv01f0P//P0L/JJFRERESgQFTiIeZrdDhw65l+nd22zLlsF//wtffw1z5pgtK85AauBAc/zFgVKlShATY3qnoqNNcLV3b9bznJyGDoWdO83QQT8/V1+hFBcOR/ZBvIiIiGSkoXoiRVzbtjBzpsm416tX7uXPnjVBU8WKpvykSfDXX3DokBnKd/fd0KxZznOuAOrWhVOn4L77oGlTWLDAjS9KvC6vwz1FRERKOwVOIsXEpZfCzTe7Vvb55+HwYdNLdc890KQJ+Fz0rz2nOVczZ8K//8IHH0BoKGzYAJ06mfPv3u2e1yPeo1T0IiIieafASaQYcXU9qKuuyhwoZSU2FnbsgHnzUhg3biXz5qWwfbvZ7+MDt94KGzeaXiofH5PAon59E5idO1eglyJeolT0IiIi+aPASaQYyW09KJvNpDGPjna9TrsdYmIs2rffS0yMlWmOS8WK8NprJtlEu3aQmAj/93/QuDH89FN6uYKkRy8MRa09RUVcnFLRi4iI5IcCJ5FiJLf1oAAmTiycCf5Nm8KSJfDZZ1C9OmzeDN26mXlUb71VtObLaP5O9lydq6ZU9CIiIhkpcBIpZnJbDyo2tvDObbOZrH0bN5qkEb6+8O23JoV6UZkvo/k7WYuPh1tugWeeca28q8NCRURESgsFTiLFkHNu0sKFMHWquXXOTfKEkBCzxtTq1RAQkHUZb8yX0fydzFJSTC9l/frw+edmX9my2Q/3hLwP9xQRESkNtI6TSDHlynpQhe3IkZyTRDjny3z7retBXX7XFrIs+OIL1+fvePvaecLSpaY3cO1a8/jKK+HNN83aXH37muApqyBzwgSt5yQiInIx9TiJSL65Og+mTx+47DIYMQI++cT0lmX1hT0vc5NSUmDlSjOnq29fE2Tdcot7211cHTxoFi9u184ETZUqwbvvwvLl0KJF9sM9fc//lPbpp+b6ioiISDr1OIlIvuVlHsymTWZ7/33z2DkcrH17c7thA9x0U+aAyjk36dNPTVKKuDj49Vf47Tez0O+FfH1d+8JfnOfv5NQj53DAO+/AI4/A8eNm34gR8NxzUKVKxnpiY6Fnz4x1VakCbdqYoZ+PPAIvvujRlyYiIlKkKXASkXxzpkffuzfrHiSbzTy/erUJdJYsMV/UV640Q+amTjUbmHWicpqblFVvUoUKcPXVph3t2kGzZmYuT3btcZo+3ax1FRiY11fsXbNmmTlcFw5HjIgwc5jCw82wvNWrzf7mzU22w9ats68vq+GeH31kAtiXXjJD+/r0cfvLEBERKZYUOIlIvjnTo2c1X+bC9OhVqsANN5gNTE/R77+bQGrJEjMXJykp9/NVqwadOpkgqV07aNgw80K/ObXH+fjtt835v/oK6tTJ98v3KGe2wKx65C4MbsqXh2efhTvvzN88pb59Yfx4k/xj6FBo1MgEoyIiIqWd5jiJSIHkJz162bJwzTXw5JPwyy/pw/dy87//mXWk7rwTLr88c9CUW3tmzoQff4TKleHPP+GKK0zwVNS5ki0QYNAgMxxy1KiCJXd4/nnTE3XqlLmeJ0/mvy4REZGSQoGTiBRYQdOjR0a6Vs7VuUk5tadrV1izxvRYJSTAzTfD6NE5Zwf0tri4nLMFOt16K1StWvDz+fqaDIU1api5Z7fdlvPQRxERkdJAgZOIuIVzvkz//uY2Lz0ezrlS2a0tZLPlfW2hnNoTEWGCqYceMo/feAPatoWtW12v35NczQLozmyB1aqZnjs/P9Mr97//ua9uERGR4qhIBE5vvPEGUVFRBAYG0rp1a/74449sy7733ntER0dTsWJFKlasSKdOnXIsLyJFn3OuFGQOni6cK+XOtYV8fc2QtDlzzNC91avN0L2ZM913Dnc4cgTmz3etrLuzBbZpkx4wPfAALF7s3vpFRESKE68HTtOnT2fcuHE88cQTrF69mqZNm9KlSxcOHjyYZflFixbRv39/Fi5cyPLly4mMjOS6665j7969Hm65iLhTfuZKuUO3bma+U9u2Zuhe375wzz3pQ/ccDli0CKZNM7cOR+G042IbN8LIkaan7cMPcy6bnx45V911l8lo6HCYYY36UysiIqWV17Pqvfrqq4wYMYJhw4YB8Pbbb/PDDz/w4Ycf8pBzHM0FPv/88wyP33//fWbOnMmCBQsYPHhwpvLnzp3j3AWTFxISEgBITk4mOTk5X212Hpff48V1utaeUVSuc48e0L07/PqrLW1toXbtLOx2KMymVa8O8+bBE0/48MordiZPhqVLUxk6NJUXX7Szd296N1h4uMWrrzro3Ttvk35MAOZgyZJwAgIcWQ5ntCxYvNjGxIk+zJmT/rtWs2YW7dql8sYbPufLpbfHZjPteOUVB6mpFqmpeXzxLnj9dfjrL1/WrbNx002pzJvnwN/f/edxp6LymS7pdJ09Q9fZc3StPaMoXee8tMFmWd6b8puUlERQUBAzZsygV69eafuHDBnC8ePH+fbbb3Ot4+TJk1StWpWvvvqKG5y5ji/w5JNPMmHChEz7p06dSlBQUIHaLyIlz8qVVZk0qQUnT/oDzj+PF44fNPsefHAFbdq4Nqlo+fIw3n+/MUeOlEnbV7nyGYYPX0ebNvEkJ9tYujSc2bPrsG1bhbQyrVrF07PnVho1OoLNlnU9Vaokctttf7vclvyKjy/LfffFkJjox/XXb2PEiHWFej4RERFPSExMZMCAAZw4cYKQkJAcy3o1cNq3bx/h4eEsW7aMNm3apO1/4IEHWLx4Mb///nuuddx1113MnTuXf/75h8AsVrPMqscpMjKSw4cP53pxspOcnMy8efPo3Lkzfn5++apDXKNr7Rm6zhnt2AGXX+5LUlLW2SpsNovwcNi8OSXXeVdff23jP/+xn89Kl7mn6D//SWXxYh/27TPPlSljMXhwKnffnUq9epnrcziy7pHzhO+/txEbawYqTJmSwoABRTfVnj7TnqHr7Bm6zp6ja+0ZRek6JyQkUKVKFZcCJ68P1SuIF154gS+++IJFixZlGTQBBAQEEBAQkGm/n59fgd8od9QhrtG19gxdZ2Pv3pwX5LUsG3v2wLPP+tGmDVSokHErU8bMO3I44L77slt/yQRK06aZqKd6dbj7brjjDhuVK9uBrKMhPz+zCLA39O4Njz4KzzwDI0f60rw5NGninba4Sp9pz9B19gxdZ8/RtfaMonCd83J+rwZOVapUwW63c+DAgQz7Dxw4QPXq1XM89pVXXuGFF15g/vz5NCnq/3OLSLHialrvZ57Jer+vrwmg/P1h377c63noIbMYcBa/8RQ5Tz4Jf/wBP/8MffrAihXmtYqIiJR0Xs2q5+/vT4sWLViwYEHavtTUVBYsWJBh6N7FXnrpJZ5++ml++uknWrZs6Ymmikgp4mpa7+bNTQrzOnVMSnPnkLmUFDh82LWgCUyvTXEImsC8xqlToWZN2LIFBg82iTvckXnQWxkMRUREXOH1oXrjxo1jyJAhtGzZkiuvvJKJEydy+vTptCx7gwcPJjw8nOeffx6AF198kccff5ypU6cSFRXF/v37AQgODiY4ONhrr0NESg7ngrx792Y9zM5mM8+vWJExM55lwenTcPw4nDgBv/xiUpvnxt3rLxW2ypVNivh27eC776BKFZPK3SkiwqzLlZcU8rNmwZgxsGdPweoREREpLF5fx6lfv3688sorPP744zRr1ow1a9bw008/Ua1aNQB27dpF/AXjZt566y2SkpLo27cvYWFhadsrr7zirZcgIiVMfhfktdkgONh84W/UyKyBFBGRuY4LyxfW+kuFrWVLuPVWc//CoAlMwNm3rwmGXDFrlil/YdCUn3pEREQKk9d7nABGjx7N6NGjs3xu0aJFGR7v2LGj8BskIqWec0HerHpBJk50rRfEGYD17WuCpAt7r3IKwIoDh8P0NmXF+TrvvBMqVoSgIDPfKyAg82a3m2ucdQINc53GjoWePYvndRIRkZKjSAROIiJFUWys+cIeF0da+u/o6Lx9gXdHAFYUxcVl7iG62KFDcM01BTuPZcHu3eZ8HTq4dozDYRYSXrIknLJlbXTsqKBLREQKToGTiEgO7HbXv7BnxxmALVyYwo8/rqFbt2Z07OhbrL/Mu5p5MCzM9CydO5dxS0nJ2/lGjYLrrzdDBFu1gqiorIdAps+V8gVa8uqr+Z8r5XAULGgWEZGSRYGTiIgH2O0QE2Nx+vReYmKaFvsv4K4mtJg6NevAMzXVrJU1fz706JF7PevXm82pcmUTRDkDqZYt4fffzbDIi4f9OedKzZjhevCkZBUiInIxBU4iIpJnrmYezC7xhY8PBAZCt26511O1Kjz9NKxeDStXwl9/wZEjMHeu2S6s0x1zpZzJKtwRgIF6rkRESgoFTiIikmfuSnzhSj1vvpkxUDl3DtatM+ngV640t3//bXqxsuOcK3XZZSaTYaVKmbeKFaF8eZMN0V3JKtzZc6UATETEuxQ4iYhIvrgr8UVe6wkISB+m5zRlCpxf/i9HW7eaLb+cAdjIkdCmjekNu3ArUya9rDt7rjR0UETE+xQ4iYhIvrkj86A76omKcq3ciy/CJZfA0aOZt2PHYPt2E9jk5r33zHax4GATQIWGmiGF2fVcAYweDVddZXq6AgOzX+9LQwdFRIoGBU4iIlIg7sg8WNB6XJ1zdd99OQcJixZBx465n69LFzOn6uBBsx04YJJdnDpltm3bcq8jPh7Cw819X18ICcm8BQfDDz8UzaGDIiKljQInEREp9tw158rVAOyHHzLWZVlw8mR6EDVzJvzvf663PyUlvecrL5xDBxs0gIYNTducW2SkuQ0PNz1a7u65EhEpbRQ4iYhIieCOOVf5DcBstvReoksvheRk1wKnBQtMOvWEhKy3hQvh009zr2fzZrNlp0oVOHHCfT1XIiKlkQInEREpMdyx2LA7AjBXe65iYkygUq5c+rC9C9Wq5Vrg9NxzUKGCaa9z273b3J45A4cP53y8s+fqiy9gwIDs51tdyOGAxYttLFkSTtmyNjp2zF/QpTlXIlJcKHASEZESxR2LDRc0WYWnhw4+8EDWdVmWSXrx3nvw0EO5t/uWW+D++81527Uzt40bZ647fa6UL9CSV1/N31wpzbkSkeJEgZOIiEgWCpr0wptDBy8sU6kStG7tWpvtdhMofvml2cAMP7z66vRAau9e0ytV0LlSmnMlIsWNAicREZFC4o507Z4cOrh+PaxebdobFwfLlpm5Vj/+aLacOOdK3XUXVKtmFiROSTFbcnLG26Qk83o050pEihMFTiIiIoXIHenaPTV0MDgY2rc3G5ggZ9269EBqwQIz9C87lmWyCrZrl6+XmaGe3bvNOd2R6l5ExB0UOImIiBQD3hg66OsLzZub7Z57YOpUGDgw93OFhpqFfX19wc8v8+2hQ/D337nXEx/v8ssTESl0CpxERERKiYL2XNWo4Vq5L7/MOchzdaHhypVdO5+IiCcocBIRESlFCtJz5epcqejogtXjNH48TJsGjRrlr70iIu7k4+0GiIiISPHgnCsFmdd6ykuadVfqCQkx86tatoQ33sg5wBIR8QQFTiIiIuIy51ypixfsjYjIWwrxnOqZORM2boSuXeHsWRg9Gm68EQ4edM9rEBHJDwVOIiIikiexsbBjB8ybl8K4cSuZNy+F7dvzvu6Ss56FC03iiYULSaunenX44QfTMxUQAN9/D02awNy5hfGKRERyp8BJRERE8sxuh5gYi/bt9xITY+V7vSXnnKv+/c3thfX4+JhsfitWmHlOBw6YXqh77zU9USIinqTASURERIq0xo1N8DR6tHk8cSK0bg3//OPVZolIKaPASURERIq8MmVg8mQzZC80FNauzZg4wuEwac6nTTO3Doe3WywiJY0CJxERESk2rr/eBE0XJo5o1QoiI83aUAMGmNuoKJg1y9utFZGSRIGTiIiIFCvOxBETJ4KvL6xaZRb0vdDevdC3r4InEXEfBU4iIiJS7Pj4mN6mypWzft657tPYsRq2JyLuocBJREREiqW4OJNpLzuWBbt3w1tvQXKya3VqrpSIZEeBk4iIiBRLFw/Py87dd0PFimZe1AsvwO+/Q0pK5nKzZpm5UZorJSJZ8fV2A0RERETyIyzMtXIhIZCQYBbPdS6gW64cREeb4KhDB7MQ7803pw/xc3LOlZoxI+8L/IpIyaLASURERIql6GiIiDDBzcUBD4DNZp7fuhU2bICFC822eDEcPw5z5pjNWTarOizLPDd2LPTsSb4X+hWR4k9D9URERKRYstth0iRz32bL+Jzz8cSJ4OcHTZrAmDHwzTdw+LDJxPff/8INN0BQUNZBk5NzrlRcXGG8ChEpLhQ4iYiISLEVG2uG0YWHZ9wfEZH98Dq7Ha64AsaNg+++g3fece1cf/9d8PaKSPGloXoiIiJSrMXGmmF0cXEmYURYmBnG5+qwuogI18rdfbfpsRo82JwzODjfTfY4hyP/10dEDAVOIiIiUuzZ7SbJQ37kNlcKICAAzp2DBQvMNnIk9OljgqiOHTMHIe4KVBwOWLzYxpIl4ZQta8vyXLmZNcsMU9yzJ31fRIQZ5qiEFyKu01A9ERERKdVymytls8HUqbB9Ozz1FFx6KSQmwqefQufOULMmPPQQrF9vjnFXWnNnPZ07+/Lqqy3p3Nk3z/XMmmWyAl4YNEF6tkClWhdxnQInERERKfVcmSsVFQWPPQabNsGyZXDnnVChgglCXnwRGjUyQVWfPgUPVAoa8KSmwpEjMGpU9tkCwWQL1CK/Iq7RUD0RERERXJ8rZbNBmzZm+9//4Icf4OOPTWrzrVuzrtsZqNx5p8ni5+dn6vXxMZvzvt1uyt51V84Bz223werVJq36sWNw9Gj6duyY2VJTc369F2YLzO8wR5HSRIGTiIiIyHl5nSsVGGh6mPr0ga+/zn3O0KFD0K1bgZoImIDp2WcLXg/AhAkmaIuOzjxUsTApYYUUNwqcRERERNzg7FnXykVGQvnyJnBITTWb877DAadOmR6j3HTqBK1bQ6VKULGiuXVuFSvCunXQtWvu9SxaBDExUKuWSXYxaBDUqZN9eXcEPEpYIcWRAicRERERNwgLc63cJ5/k3Ku1aJFJKJGbRx7JuZ5q1XLOFmizQWio6QGbOdMkv5gwwWzt2sGQIXDTTSbIc3JHwOOcv3Vxm5zzt7Jbf0vE25QcQkRERMQNnGnNsxvuZrOZ3qboaM/Uk1u2QIC33oIpU+DAAfj8c7juOjNs79dfYcQIqF4d+veHn36Cr74qeIY+h8MEXu5MWOFwmGBz2jRz6+1kF0WtPe5SUl9XXqjHSURERMQNnIFK374mMLkwOHAGKhMn5j6szV31QHq2wKx6iSZOTO/ZCQoyqdMHDDCB0Oefm4QX69fDF1+Yzccn+4DHZjPnaNfODDVMSIATJzJuCQlm+ODFgdfFdeUlYYU7h/xpzazsldTXlVcKnERERETcxNVAxVP1OOtyJVugU3g4PPAA3H+/ydz38cdmS0jI/hyWZdpZrZrr7crJwIFw1VXQsCE0aGC2yy4zAZ6TO4f8pQcGvkBLXn3V+0MQi0ryDA2tTKfASURERMSN8hqo5FbPwoUp/PjjGrp1a0bHjr75+vKc12yBYHqRWrQwW6tWJnGEK4KCzLyo8uUhJCT9fvnyJvj66qvc69i3z3xhv3D4n81mFht2BlEff5xzD9jYseb65Xa93BEY5DYEMS/tcbapKPTwuPt1FXcKnERERETcLD+BSnb1xMRYnD69l5iYpl77choZ6Vq5efNMtr/sOBywfHnOCSuqV4f33zcLDa9fDxs2mO3IEdixw2w//phzO5xD/oYNg6ZNzULFF27ly5vb4ODcA4MxY0zgeOqUSQN/4fBD5+P1610bgvjJJyYYK1cu+7JFqYcnLs69QyuLOwVOIiIiIpIjZ8KKnAKeiIjcswG6Mn/r9dehe3ezXejQofQg6uuvYe7c3Nv96admyy/nEMRLLsl/HRe69VazVahg6qxZ09w6t/BwGD3avT08eR3y53DAX3/BkiVmrpsrdu50rVxxp8BJRERERHLkjYQVFwsNNVv79maoniuBU69eULas6Rly9g457586lfvxTjZbxp6qC4cfVqhg1t367LPc6ylbFk6fTm/D2rWutwHSe3gWL4Zrrsm9vCtD/pKSYNUqEygtXgxLl+Y8ny0rd94Jc+aYz0f37uZ1ZscdSTi8RYGTiIiIiOTKmwkrLuZqD9iMGdnXmZJivuz37Jn7+RYsyLk3zZmqO7f2bN9uAqfdu2HXLrPt3Jl+f8MGOHw49/Zcdx3Urm22WrXM5rxfu7ZZADmnIX99+sDNN5tzLV8OZ85kLBMSYjIktmtn3ttDh7J+XWCu79mz8OWXZitTxqwN1rcvXH+9qcvJHUk4vEmBk4iIiIi4xF2JL6Bg88Dc0QPm62u+2LsSgLVv7772hIRAo0Zmu5irix87HLB5s9myEhJigqGc1sv68sv0fVWqmNfo3Jo0Sb92l12W8+uaPt0MM5wxw2zbtqUn9vD3hy5d0o8fMqRozN3KLwVOIiIiIuIydyW+KCh39IAVhSGIF3KlJy083ARYu3aZIGX7drM57x844PpQu3vvheHDTabC7BZcdvV1tWoFL7xg5kfNmGGyJ27aBN99Z7bsFKfsfAqcRERERKRYckcPWFEaguhKIDdpEtSpY7aseqcSE+GNN8xaXLlp1cqsleWu12WzQbNmZnv6afjnH3NtP/7YZEPMTnHJzqfASUTk/9u795iq6z+O46+DwBFUBON20DRIIm+w1KJT2Q2mkCtNW2WsYbUchs2uK12pXTZdNbut6J5t9ZPSpZnTirzQMjRFCbzE0lFaQpSmIIY3Pr8/HN/f7yjyBcRzwedjO+vw/XzP1/f35Wff9vb7PR8AAAGrM+6A+fp3Zp1ay9k0cuHhJxuitnC52l5Xe8/L4ZCGDj35SkmR7rrL/jPV1W0/vi/QOAEAAOC85y+/M0vy3uIZo0Z1bt1n0tYGrT2NnC/QOAEAAAB+xteLZ3Qmf2vkOirI1wUAAAAA6FzNj/z17eu5vXmZdm+uYNfcyEmnL0Lhi0auo2icAAAAgC5owoSTizKsWSP95z8n/1tV5Ztlv/2pkesoHtUDAAAAuih/WT5e6txFOHyBxgkAAACAV/jTIhztxaN6AAAAAGCDxgkAAAAAbNA4AQAAAIANGicAAAAAsEHjBAAAAAA2aJwAAAAAwAaNEwAAAADYoHECAAAAABs0TgAAAABgg8YJAAAAAGwE+7oAbzPGSJLq6uo6fIxjx47p8OHDqqurU0hISGeVhhaQtXeQs3eQs/eQtXeQs3eQs/eQtXf4U87NPUFzj9Ca865xqq+vlyRdeOGFPq4EAAAAgD+or69X7969W93HYdrSXnUhTU1N2rt3r3r16iWHw9GhY9TV1enCCy/Unj17FBER0ckV4v+RtXeQs3eQs/eQtXeQs3eQs/eQtXf4U87GGNXX1yshIUFBQa1/i+m8u+MUFBSkfv36dcqxIiIifP6Xfb4ga+8gZ+8gZ+8ha+8gZ+8gZ+8ha+/wl5zt7jQ1Y3EIAAAAALBB4wQAAAAANmicOsDpdGr27NlyOp2+LqXLI2vvIGfvIGfvIWvvIGfvIGfvIWvvCNScz7vFIQAAAACgvbjjBAAAAAA2aJwAAAAAwAaNEwAAAADYoHECAAAAABs0Th3wxhtv6KKLLlL37t2Vnp6uH3/80dclBbQ5c+bI4XB4vC699FJrvLGxUfn5+brgggvUs2dPTZw4UX/++acPKw4M3333nW6++WYlJCTI4XBo6dKlHuPGGM2aNUsul0thYWHKzMzUL7/84rHP/v37lZOTo4iICEVGRuq+++7ToUOHvHgWgcEu68mTJ582x7Oysjz2IWt7c+fO1eWXX65evXopNjZW48ePV2Vlpcc+bble7N69W2PHjlV4eLhiY2P1+OOP6/jx4948Fb/Wlpyvv/760+Z0Xl6exz7k3LqCggKlpqZavwDU7XZr5cqV1jhzufPYZc18PjfmzZsnh8Ohhx56yNoW6POaxqmdPv30Uz3yyCOaPXu2Nm/erLS0NI0ZM0a1tbW+Li2gDRkyRNXV1dbr+++/t8Yefvhhffnll1q0aJGKi4u1d+9eTZgwwYfVBoaGhgalpaXpjTfeaHH8hRde0Guvvaa33npLGzZsUI8ePTRmzBg1NjZa++Tk5Gjbtm0qKirS8uXL9d1332nKlCneOoWAYZe1JGVlZXnM8YULF3qMk7W94uJi5efna/369SoqKtKxY8c0evRoNTQ0WPvYXS9OnDihsWPH6ujRo/rhhx/00UcfacGCBZo1a5YvTskvtSVnSbr//vs95vQLL7xgjZGzvX79+mnevHkqLS3Vpk2bdOONN2rcuHHatm2bJOZyZ7LLWmI+d7aNGzfq7bffVmpqqsf2gJ/XBu1yxRVXmPz8fOvnEydOmISEBDN37lwfVhXYZs+ebdLS0locO3DggAkJCTGLFi2ytu3YscNIMiUlJV6qMPBJMkuWLLF+bmpqMvHx8ebFF1+0th04cMA4nU6zcOFCY4wx27dvN5LMxo0brX1WrlxpHA6H+eOPP7xWe6A5NWtjjMnNzTXjxo0742fIumNqa2uNJFNcXGyMadv1YsWKFSYoKMjU1NRY+xQUFJiIiAhz5MgR755AgDg1Z2OMue6668z06dPP+Bly7pioqCjz3nvvMZe9oDlrY5jPna2+vt4kJyeboqIij2y7wrzmjlM7HD16VKWlpcrMzLS2BQUFKTMzUyUlJT6sLPD98ssvSkhIUFJSknJycrR7925JUmlpqY4dO+aR+aWXXqr+/fuT+VmoqqpSTU2NR669e/dWenq6lWtJSYkiIyM1cuRIa5/MzEwFBQVpw4YNXq850K1du1axsbFKSUnR1KlTtW/fPmuMrDvm4MGDkqQ+ffpIatv1oqSkRMOGDVNcXJy1z5gxY1RXV+fxr8/4n1NzbvbJJ58oOjpaQ4cO1YwZM3T48GFrjJzb58SJEyosLFRDQ4Pcbjdz+Rw6NetmzOfOk5+fr7Fjx3rMX6lrXKODfV1AIPn777914sQJj79MSYqLi9PPP//so6oCX3p6uhYsWKCUlBRVV1frmWee0ahRo7R161bV1NQoNDRUkZGRHp+Ji4tTTU2NbwruApqza2kuN4/V1NQoNjbWYzw4OFh9+vQh+3bKysrShAkTlJiYqF27dmnmzJnKzs5WSUmJunXrRtYd0NTUpIceekhXX321hg4dKkltul7U1NS0OO+bx+CppZwl6a677tKAAQOUkJCg8vJyPfHEE6qsrNTnn38uiZzbqqKiQm63W42NjerZs6eWLFmiwYMHq6ysjLncyc6UtcR87kyFhYXavHmzNm7ceNpYV7hG0zjB57Kzs633qampSk9P14ABA/TZZ58pLCzMh5UBnePOO++03g8bNkypqam6+OKLtXbtWmVkZPiwssCVn5+vrVu3enwfEp3vTDn///fvhg0bJpfLpYyMDO3atUsXX3yxt8sMWCkpKSorK9PBgwe1ePFi5ebmqri42NdldUlnynrw4MHM506yZ88eTZ8+XUVFRerevbuvyzkneFSvHaKjo9WtW7fTVv/4888/FR8f76Oqup7IyEhdcskl2rlzp+Lj43X06FEdOHDAYx8yPzvN2bU2l+Pj409b9OT48ePav38/2Z+lpKQkRUdHa+fOnZLIur2mTZum5cuXa82aNerXr5+1vS3Xi/j4+BbnffMY/udMObckPT1dkjzmNDnbCw0N1cCBAzVixAjNnTtXaWlpevXVV5nL58CZsm4J87ljSktLVVtbq+HDhys4OFjBwcEqLi7Wa6+9puDgYMXFxQX8vKZxaofQ0FCNGDFCq1atsrY1NTVp1apVHs/J4uwcOnRIu3btksvl0ogRIxQSEuKReWVlpXbv3k3mZyExMVHx8fEeudbV1WnDhg1Wrm63WwcOHFBpaam1z+rVq9XU1GT9TwUd8/vvv2vfvn1yuVySyLqtjDGaNm2alixZotWrVysxMdFjvC3XC7fbrYqKCo9GtaioSBEREdZjO+c7u5xbUlZWJkkec5qc26+pqUlHjhxhLntBc9YtYT53TEZGhioqKlRWVma9Ro4cqZycHOt9wM9rX69OEWgKCwuN0+k0CxYsMNu3bzdTpkwxkZGRHqt/oH0effRRs3btWlNVVWXWrVtnMjMzTXR0tKmtrTXGGJOXl2f69+9vVq9ebTZt2mTcbrdxu90+rtr/1dfXmy1btpgtW7YYSWb+/Plmy5Yt5rfffjPGGDNv3jwTGRlpvvjiC1NeXm7GjRtnEhMTzb///msdIysry1x22WVmw4YN5vvvvzfJyclm0qRJvjolv9Va1vX19eaxxx4zJSUlpqqqynz77bdm+PDhJjk52TQ2NlrHIGt7U6dONb179zZr16411dXV1uvw4cPWPnbXi+PHj5uhQ4ea0aNHm7KyMvPVV1+ZmJgYM2PGDF+ckl+yy3nnzp3m2WefNZs2bTJVVVXmiy++MElJSebaa6+1jkHO9p588klTXFxsqqqqTHl5uXnyySeNw+Ew33zzjTGGudyZWsua+XxunbpiYaDPaxqnDnj99ddN//79TWhoqLniiivM+vXrfV1SQLvjjjuMy+UyoaGhpm/fvuaOO+4wO3futMb//fdf88ADD5ioqCgTHh5ubr31VlNdXe3DigPDmjVrjKTTXrm5ucaYk0uSP/300yYuLs44nU6TkZFhKisrPY6xb98+M2nSJNOzZ08TERFh7rnnHlNfX++Ds/FvrWV9+PBhM3r0aBMTE2NCQkLMgAEDzP3333/aP7aQtb2WMpZkPvzwQ2uftlwvfv31V5OdnW3CwsJMdHS0efTRR82xY8e8fDb+yy7n3bt3m2uvvdb06dPHOJ1OM3DgQPP444+bgwcPehyHnFt37733mgEDBpjQ0FATExNjMjIyrKbJGOZyZ2ota+bzuXVq4xTo89phjDHeu78FAAAAAIGH7zgBAAAAgA0aJwAAAACwQeMEAAAAADZonAAAAADABo0TAAAAANigcQIAAAAAGzROAAAAAGCDxgkAAAAAbNA4AQDQCofDoaVLl/q6DACAj9E4AQD81uTJk+VwOE57ZWVl+bo0AMB5JtjXBQAA0JqsrCx9+OGHHtucTqePqgEAnK+44wQA8GtOp1Px8fEer6ioKEknH6MrKChQdna2wsLClJSUpMWLF3t8vqKiQjfeeKPCwsJ0wQUXaMqUKTp06JDHPh988IGGDBkip9Mpl8uladOmeYz//fffuvXWWxUeHq7k5GQtW7bMGvvnn3+Uk5OjmJgYhYWFKTk5+bRGDwAQ+GicAAAB7emnn9bEiRP1008/KScnR3feead27NghSWpoaNCYMWMUFRWljRs3atGiRfr22289GqOCggLl5+drypQpqqio0LJlyzRw4ECPP+OZZ57R7bffrvLyct10003KycnR/v37rT9/+/btWrlypXbs2KGCggJFR0d7LwAAgFc4jDHG10UAANCSyZMn6+OPP1b37t09ts+cOVMzZ86Uw+FQXl6eCgoKrLErr7xSw4cP15tvvql3331XTzzxhPbs2aMePXpIklasWKGbb75Ze/fuVVxcnPr27at77rlHzz//fIs1OBwOPfXUU3ruuecknWzGevbsqZUrVyorK0u33HKLoqOj9cEHH5yjFAAA/oDvOAEA/NoNN9zg0RhJUp8+faz3brfbY8ztdqusrEyStGPHDqWlpVlNkyRdffXVampqUmVlpRwOh/bu3auMjIxWa0hNTbXe9+jRQxEREaqtrZUkTZ06VRMnTtTmzZs1evRojR8/XldddVWHzhUA4L9onAAAfq1Hjx6nPTrXWcLCwtq0X0hIiMfPDodDTU1NkqTs7Gz99ttvWrFihYqKipSRkaH8/Hy99NJLnV4vAMB3+I4TACCgrV+//rSfBw0aJEkaNGiQfvrpJzU0NFjj69atU1BQkFJSUtSrVy9ddNFFWrVq1VnVEBMTo9zcXH388cd65ZVX9M4775zV8QAA/oc7TgAAv3bkyBHV1NR4bAsODrYWYFi0aJFGjhypa665Rp988ol+/PFHvf/++5KknJwczZ49W7m5uZozZ47++usvPfjgg7r77rsVFxcnSZozZ47y8vIUGxur7Oxs1dfXa926dXrwwQfbVN+sWbM0YsQIDRkyREeOHNHy5cutxg0A0HXQOAEA/NpXX30ll8vlsS0lJUU///yzpJMr3hUWFuqBBx6Qy+XSwoULNXjwYElSeHi4vv76a02fPl2XX365wsPDNXHiRM2fP986Vm5urhobG/Xyyy/rscceU3R0tG677bY21xcaGqoZM2bo119/VVhYmEaNGqXCwsJOOHMAgD9hVT0AQMByOBxasmSJxo8f7+tSAABdHN9xAgAAAAAbNE4AAAAAYIPvOAEAAhZPmwMAvIU7TgAAAABgg8YJAAAAAGzQOAEAAACADRonAAAAALBB4wQAAAAANmicAAAAAMAGjRMAAAAA2KBxAgAAAAAb/wVfpj/AC9rnZQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure you have the correct lengths\n",
    "x_epochs = range(10, num_epochs + 1, 10)  # This should still be (40,)\n",
    "# Assuming you adjusted your train_losses and val_losses lists accordingly\n",
    "\n",
    "# Plotting the training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_epochs, train_losses, label='Training Loss', color='blue', marker='o')\n",
    "plt.plot(x_epochs, val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa3e1c-c184-44e5-9e22-3a23785e27ca",
   "metadata": {},
   "source": [
    "# Custom Dataset class for multi-channel image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41dec30e-492b-4c5b-bf7c-ee71dd93f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiChannelImageDataset(Dataset):\n",
    "    def __init__(self, image_arrays):\n",
    "        # Each element in image_arrays is a tuple: (speed, cos(direction), sin(direction), latitude, longitude)\n",
    "        self.images = image_arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Stack the channels along a new dimension to create a multi-channel image\n",
    "        image = np.stack(self.images[idx], axis=0)  # Shape: (5, H, W)\n",
    "        return torch.tensor(image, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b4e15e6-8240-4fb2-bb98-60b732527d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  9.112082 ,   8.611857 ,   8.684312 , ...,  16.634079 ,\n",
       "                 nan,         nan],\n",
       "        [  8.777194 ,   8.815788 ,   8.354136 , ...,  16.917137 ,\n",
       "                 nan,         nan],\n",
       "        [  9.870857 ,   9.256445 ,   8.219499 , ...,  16.885883 ,\n",
       "                 nan,         nan],\n",
       "        ...,\n",
       "        [  8.042099 ,   7.2268963,   7.3592315, ...,  17.846361 ,\n",
       "                 nan,         nan],\n",
       "        [  7.2899995,   7.4636827,   7.2250953, ...,  17.870579 ,\n",
       "                 nan,         nan],\n",
       "        [  7.188838 ,   7.329113 ,   7.0611677, ...,  17.625313 ,\n",
       "                 nan,         nan]],\n",
       "\n",
       "       [[191.50705  , 191.51346  , 191.51987  , ..., 158.12077  ,\n",
       "                 nan,         nan],\n",
       "        [191.50699  , 191.51338  , 191.51979  , ..., 158.12125  ,\n",
       "                 nan,         nan],\n",
       "        [199.00691  , 194.01332  , 189.01973  , ..., 158.12172  ,\n",
       "                 nan,         nan],\n",
       "        ...,\n",
       "        [198.99504  , 196.5018   , 196.50856  , ..., 228.19794  ,\n",
       "                 nan,         nan],\n",
       "        [196.49496  , 196.50172  , 196.50848  , ..., 228.19841  ,\n",
       "                 nan,         nan],\n",
       "        [196.49489  , 196.50165  , 196.5084   , ..., 228.19888  ,\n",
       "                 nan,         nan]],\n",
       "\n",
       "       [[ 35.73571  ,  35.737404 ,  35.739094 , ...,  36.131523 ,\n",
       "          36.132965 ,  36.134407 ],\n",
       "        [ 35.744705 ,  35.7464   ,  35.74809  , ...,  36.140514 ,\n",
       "          36.141956 ,  36.1434   ],\n",
       "        [ 35.753704 ,  35.755394 ,  35.757088 , ...,  36.149506 ,\n",
       "          36.150948 ,  36.15239  ],\n",
       "        ...,\n",
       "        [ 37.21116  ,  37.212856 ,  37.21455  , ...,  37.605297 ,\n",
       "          37.606728 ,  37.608162 ],\n",
       "        [ 37.220154 ,  37.221848 ,  37.223545 , ...,  37.61428  ,\n",
       "          37.615715 ,  37.617146 ],\n",
       "        [ 37.229145 ,  37.230843 ,  37.232536 , ...,  37.62327  ,\n",
       "          37.624702 ,  37.626133 ]],\n",
       "\n",
       "       [[-74.00143  , -73.99057  , -73.97971  , ..., -71.25149  ,\n",
       "         -71.240524 , -71.22955  ],\n",
       "        [-74.00357  , -73.99271  , -73.98185  , ..., -71.25332  ,\n",
       "         -71.24235  , -71.231384 ],\n",
       "        [-74.00571  , -73.99485  , -73.983986 , ..., -71.25514  ,\n",
       "         -71.24418  , -71.23321  ],\n",
       "        ...,\n",
       "        [-74.35651  , -74.34544  , -74.33437  , ..., -71.55322  ,\n",
       "         -71.542046 , -71.53086  ],\n",
       "        [-74.35869  , -74.347626 , -74.33656  , ..., -71.55508  ,\n",
       "         -71.54389  , -71.532715 ],\n",
       "        [-74.36088  , -74.349815 , -74.338745 , ..., -71.55693  ,\n",
       "         -71.545746 , -71.53456  ]]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(all_arrays[0], axis = 0)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "216acdbc-ddb1-4799-8c82-2d8aa4b67e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42251, 5)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bcb6027-3b12-4ec0-8dc4-e9906f67aee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 255)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_arrays[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e85bd-60dc-4661-90b5-b8faacc07cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff60a9-17b5-4eb3-a5e1-3df42d287f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b57c5-1b0e-4091-987e-a8b47e0ae999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0fb85b-cfe3-49a5-9bd5-d1c8b9696bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fd77388-d511-4555-8691-5ff7b8b289d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Summary:\n",
      "Shape: ((167, 255), (167, 255), (167, 255), (167, 255)), Count: 2\n",
      "Shape: ((150, 253), (150, 253), (150, 253), (150, 253)), Count: 1\n",
      "Shape: ((194, 252), (194, 252), (194, 252), (194, 252)), Count: 1\n",
      "Shape: ((167, 253), (167, 253), (167, 253), (167, 253)), Count: 1\n",
      "Shape: ((169, 252), (169, 252), (169, 252), (169, 252)), Count: 1\n",
      "Shape: ((193, 252), (193, 252), (193, 252), (193, 252)), Count: 1\n",
      "Shape: ((193, 257), (193, 257), (193, 257), (193, 257)), Count: 1\n",
      "Shape: ((194, 253), (194, 253), (194, 253), (194, 253)), Count: 1\n",
      "Shape: ((193, 255), (193, 255), (193, 255), (193, 255)), Count: 1\n",
      "Shape: ((167, 254), (167, 254), (167, 254), (167, 254)), Count: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_shapes(data):\n",
    "    # Create a Counter to hold the shape counts\n",
    "    shape_counts = Counter()\n",
    "    \n",
    "    # Iterate through each tuple in all_arrays\n",
    "    for arrays in data:\n",
    "        # Get the shapes of each element in the tuple\n",
    "        shapes = tuple(arr.shape for arr in arrays)\n",
    "        # Update the Counter with this shape combination\n",
    "        shape_counts[shapes] += 1\n",
    "    \n",
    "    return shape_counts\n",
    "\n",
    "# Example usage\n",
    "shape_summary = count_shapes(all_arrays)\n",
    "print(\"Shape Summary:\")\n",
    "for shape, count in shape_summary.items():\n",
    "    print(f\"Shape: {shape}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66cfa5-544a-40e5-9496-56d86bab7437",
   "metadata": {},
   "source": [
    "# all_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1e2ed1a-620e-4a81-8fff-f763ea576756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiChannelImageDataset(Dataset):\n",
    "    def __init__(self, scaled_data_list):\n",
    "        self.images = scaled_data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Access the tuple of arrays for the given index\n",
    "        item = self.images[idx]\n",
    "        \n",
    "        # Check if the item is a tuple and has the correct structure\n",
    "        if not isinstance(item, tuple) or len(item) != 5:\n",
    "            raise ValueError(f\"Expected a tuple of 5 arrays, but got {type(item)} with length {len(item)}\")\n",
    "\n",
    "        # Stack the channels along a new dimension to create a multi-channel image\n",
    "        try:\n",
    "            image = np.stack(item, axis=0)  # Shape: (5, H, W)\n",
    "        except TypeError as e:\n",
    "            print(f\"Error stacking item at index {idx}: {e}\")\n",
    "            print(f\"Item type: {type(item)}, content: {item}\")\n",
    "            raise e\n",
    "            \n",
    "        return torch.tensor(image, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4bd5db7-80ed-4289-a69f-00649afea2af",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'ellipsis' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     79\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     82\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m     83\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(output, data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cris_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cris_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cris_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[29], line 27\u001b[0m, in \u001b[0;36mMultiChannelImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check if the item is a tuple and has the correct structure\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a tuple of 5 arrays, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Stack the channels along a new dimension to create a multi-channel image\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'ellipsis' has no len()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Sample input data (replace with your actual data)\n",
    "# all_arrays contains tuples of (speed, cos(direction), sin(direction), lat, lon)\n",
    "# Each element is expected to have shape (H, W), e.g., (167, 255)\n",
    "\n",
    "# Example: all_arrays = [(speed1, cos_dir1, sin_dir1, lat1, lon1), (speed2, cos_dir2, sin_dir2, lat2, lon2), ...]\n",
    "\n",
    "# Step 1: Define Dataset Class\n",
    "class MultiChannelImageDataset(Dataset):\n",
    "    def __init__(self, scaled_data_list):\n",
    "        self.images = scaled_data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Access the tuple of arrays for the given index\n",
    "        item = self.images[idx]\n",
    "        \n",
    "        # Check if the item is a tuple and has the correct structure\n",
    "        if not isinstance(item, tuple) or len(item) != 5:\n",
    "            raise ValueError(f\"Expected a tuple of 5 arrays, but got {type(item)} with length {len(item)}\")\n",
    "\n",
    "        # Stack the channels along a new dimension to create a multi-channel image\n",
    "        try:\n",
    "            image = np.stack(item, axis=0)  # Shape: (5, H, W)\n",
    "        except TypeError as e:\n",
    "            print(f\"Error stacking item at index {idx}: {e}\")\n",
    "            print(f\"Item type: {type(item)}, content: {item}\")\n",
    "            raise e\n",
    "            \n",
    "        return torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "# Step 2: Define the Model\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(5, 16, kernel_size=3, stride=2, padding=1),  # 5 input channels for speed, cos, sin, lat, lon\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=7)  # Kernel size adjusted to work with varied input dimensions\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 5, kernel_size=3, stride=2, padding=1, output_padding=1),  # 5 output channels\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: Prepare Dataset and DataLoader\n",
    "all_arrays = [...]  # Load your data here\n",
    "dataset = MultiChannelImageDataset(all_arrays)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Step 4: Initialize Model, Loss Function, and Optimizer\n",
    "model = ConvAutoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 5: Training Loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# Step 6: Feature Extraction Function\n",
    "def extract_features(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(data)\n",
    "    return features\n",
    "\n",
    "# Step 7: Extract Features for Each Image\n",
    "image_features = []\n",
    "for data in dataloader:\n",
    "    features = extract_features(model, data)\n",
    "    image_features.append(features)\n",
    "\n",
    "# Now, image_features contains the feature representations for each input sample\n",
    "print(\"Feature extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "464f1548-b285-421d-95e0-a8d8c92ae321",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'ellipsis' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 79\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     78\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m     82\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(output, data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cris_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cris_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cris_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[25], line 27\u001b[0m, in \u001b[0;36mMultiChannelImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check if the item is a tuple and has the correct structure\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a tuple of 5 arrays, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Stack the channels along a new dimension to create a multi-channel image\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'ellipsis' has no len()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Sample data preparation (replace `all_arrays` with your actual data)\n",
    "# all_arrays should contain tuples of (speed, cos(direction), sin(direction), lat, lon)\n",
    "# Each element in the tuple should be a numpy array with the same shape, e.g., (H, W)\n",
    "# Example:\n",
    "# all_arrays = [(speed_array, cos_dir_array, sin_dir_array, lat_array, lon_array), ...]\n",
    "\n",
    "# Custom Dataset class for multi-channel image data\n",
    "class MultiChannelImageDataset(Dataset):\n",
    "    def __init__(self, scaled_data_list):\n",
    "        self.images = scaled_data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Access the tuple of arrays for the given index\n",
    "        item = self.images[idx]\n",
    "        \n",
    "        # Check if the item is a tuple and has the correct structure\n",
    "        if not isinstance(item, tuple) or len(item) != 5:\n",
    "            raise ValueError(f\"Expected a tuple of 5 arrays, but got {type(item)} with length {len(item)}\")\n",
    "\n",
    "        # Stack the channels along a new dimension to create a multi-channel image\n",
    "        try:\n",
    "            image = np.stack(item, axis=0)  # Shape: (5, H, W)\n",
    "        except TypeError as e:\n",
    "            print(f\"Error stacking item at index {idx}: {e}\")\n",
    "            print(f\"Item type: {type(item)}, content: {item}\")\n",
    "            raise e\n",
    "            \n",
    "        return torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "# Define the Convolutional Autoencoder\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(5, 16, kernel_size=3, stride=2, padding=1),  # 5 input channels for each variable\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=7)  # Adapt kernel size for variable image sizes\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 5, kernel_size=3, stride=2, padding=1, output_padding=1),  # 5 output channels\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the dataset and dataloader\n",
    "dataset = MultiChannelImageDataset(all_arrays)  # all_arrays should contain tuples of (speed, cos(dir), sin(dir), lat, lon)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ConvAutoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Function to extract features from the encoder\n",
    "def extract_features(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(data)\n",
    "    return features\n",
    "\n",
    "# Extract features for each image in the dataset\n",
    "image_features = []\n",
    "for data in dataloader:\n",
    "    features = extract_features(model, data)\n",
    "    image_features.append(features)\n",
    "\n",
    "# Print feature shapes as a check\n",
    "for i, features in enumerate(image_features):\n",
    "    print(f\"Image {i+1} features shape: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0786b344-a745-4c1c-ba74-3c36588f60f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crist\\anaconda3\\envs\\cris_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([1, 1, 194, 252])) that is different to the input size (torch.Size([1, 1, 196, 252])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (196) must match the size of tensor b (194) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 64\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Move data to device if GPU is available\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# data = data.to(device)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m---> 64\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, data)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cris_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cris_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cris_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cris_env\\Lib\\site-packages\\torch\\nn\\functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[0;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3794\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cris_env\\Lib\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (196) must match the size of tensor b (194) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Dummy autoencoder model with convolutional layers\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=7)  # Adapt kernel for variable size\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Dummy dataset\n",
    "class VariableSizeImageDataset(Dataset):\n",
    "    def __init__(self, image_arrays):\n",
    "        self.images = image_arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        # Add channel dimension for PyTorch (e.g., grayscale: 1 channel)\n",
    "        return torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "# Assuming all_arrays contains numpy arrays of variable sizes\n",
    "dataset = VariableSizeImageDataset([arr[0] for arr in all_arrays])  # use only 'speed' as example\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)  # Batch size = 1 for variable sizes\n",
    "\n",
    "# Model, loss function, optimizer\n",
    "model = ConvAutoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for data in dataloader:\n",
    "        # Move data to device if GPU is available\n",
    "        # data = data.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6e3ad-1ae6-4e80-9ccd-157b7aedc724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cris_env)",
   "language": "python",
   "name": "cris_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
